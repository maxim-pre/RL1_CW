{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-02T11:20:43.181662Z",
     "start_time": "2025-12-02T11:20:43.005759Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras.models import clone_model\n",
    "import gymnasium as gym"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:20:46.177002Z",
     "start_time": "2025-12-02T11:20:46.139513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TD3Agent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 critic_network1,\n",
    "                 critic_network2,\n",
    "                 actor_network,\n",
    "                 critic_learning_rate=1e-3,\n",
    "                 actor_learning_rate=1e-3,\n",
    "                 discount_factor=0.99,\n",
    "                 minibatch_size=100,\n",
    "                 tau=0.005,\n",
    "                 exploratory_noise_std=0.1,\n",
    "                 policy_noise=0.1,\n",
    "                 noise_clip=0.5,\n",
    "                 policy_delay=2,\n",
    "                 warm_up=1000,\n",
    "                 max_buffer_length=1_000_000):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.action_min, self.action_max = env.action_space.low, env.action_space.high\n",
    "        self.max_reward = env.spec.reward_threshold\n",
    "\n",
    "        # Initialize critic network 1 and target critic network 1.\n",
    "        self.critic_network1 = clone_model(critic_network1)\n",
    "        self.critic_network1.set_weights(critic_network1.get_weights())\n",
    "\n",
    "        self.target_critic_network1 = clone_model(critic_network1)\n",
    "        self.target_critic_network1.set_weights(critic_network1.get_weights())\n",
    "\n",
    "        # Initialize critic network 2 and target critic network 2.\n",
    "        self.critic_network2 = clone_model(critic_network2)\n",
    "        self.critic_network2.set_weights(critic_network2.get_weights())\n",
    "\n",
    "        self.target_critic_network2 = clone_model(critic_network2)\n",
    "        self.target_critic_network2.set_weights(critic_network2.get_weights())\n",
    "\n",
    "        # Initialize actor network and target actor network.\n",
    "        self.actor_network = clone_model(actor_network)\n",
    "        self.actor_network.set_weights(actor_network.get_weights())\n",
    "\n",
    "        self.target_actor_network = clone_model(actor_network)\n",
    "        self.target_actor_network.set_weights(actor_network.get_weights())\n",
    "\n",
    "        # Initialize network optimizers.\n",
    "        self.critic1_optimizer = Adam(learning_rate=critic_learning_rate)\n",
    "        self.critic2_optimizer = Adam(learning_rate=critic_learning_rate)\n",
    "        self.actor_optimizer = Adam(learning_rate=actor_learning_rate)\n",
    "\n",
    "        # Initialize hyperparameters.\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.exploratory_noise_std = exploratory_noise_std\n",
    "        self.policy_noise = policy_noise\n",
    "        self.noise_clip = noise_clip\n",
    "        self.policy_delay = policy_delay\n",
    "        self.warm_up = warm_up\n",
    "\n",
    "        # Initialize buffer.\n",
    "        self.buffer_width = 2 * self.state_size + self.action_size + 2\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.replay_buffer = np.zeros((self.max_buffer_length, self.buffer_width), dtype=np.float32)\n",
    "        self.buffer_write_idx = 0\n",
    "        self.buffer_fullness = 0\n",
    "\n",
    "        # Initialize minibatch slicers.\n",
    "        self.state_slice = slice(0, self.state_size)\n",
    "        self.state_action_slice = slice(0, self.state_size + self.action_size)\n",
    "        self.reward_slice = slice(self.state_size + self.action_size, self.state_size + self.action_size + 1)\n",
    "        self.next_state_slice = slice(self.state_size + self.action_size + 1, 2 * self.state_size + self.action_size + 1)\n",
    "        self.terminal_slice = slice(2 * self.state_size + self.action_size + 1, 2 * self.state_size + self.action_size + 2)\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action at the given state.\"\"\"\n",
    "        # Input check.\n",
    "        assert state.ndim == 1 and state.shape[0] == self.state_size\n",
    "        # Do forward pass.\n",
    "        action = self.actor_network(np.expand_dims(state, axis=0), training=False).numpy()[0]\n",
    "        # Add exploratory noise.\n",
    "        noise = np.random.normal(0, self.exploratory_noise_std, self.action_size)\n",
    "        action += noise\n",
    "        action = np.clip(action, self.action_min, self.action_max)\n",
    "        return action\n",
    "\n",
    "    def save_transition(self, state: np.ndarray, action: np.ndarray, reward: float, new_state: np.ndarray, terminal: bool):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        # Input check.\n",
    "        assert state.ndim == 1 and state.shape[0] == self.state_size\n",
    "        assert action.ndim == 1 and action.shape[0] == self.action_size\n",
    "        assert new_state.ndim == 1 and new_state.shape[0] == self.state_size\n",
    "\n",
    "        # Save transition.\n",
    "        transition = np.concatenate((state, action, [reward], new_state, [1.0 if terminal else 0.0]), dtype=np.float32)\n",
    "        self.replay_buffer[self.buffer_write_idx] = transition\n",
    "\n",
    "        # Update write index and fullness.\n",
    "        self.buffer_write_idx = (self.buffer_write_idx + 1) % self.max_buffer_length\n",
    "        self.buffer_fullness = min(self.buffer_fullness + 1, self.max_buffer_length)\n",
    "\n",
    "    def sample_minibatch(self) -> tf.Tensor:\n",
    "        \"\"\"Sample a minibatch from the replay buffer.\"\"\"\n",
    "        indices = np.random.choice(self.buffer_fullness, size=self.minibatch_size, replace=False)\n",
    "        minibatch = self.replay_buffer[indices]\n",
    "        return tf.convert_to_tensor(minibatch, dtype=tf.float32)\n",
    "\n",
    "    @tf.function\n",
    "    def update_critic_networks(self, minibatch: tf.Tensor):\n",
    "        \"\"\"Update critic networks\"\"\"\n",
    "        mb_state_actions = minibatch[:, self.state_action_slice]\n",
    "        mb_rewards = minibatch[:, self.reward_slice]\n",
    "        mb_next_states = minibatch[:, self.next_state_slice]\n",
    "        mb_terminals = minibatch[:, self.terminal_slice]\n",
    "\n",
    "        next_actions = self.target_actor_network(mb_next_states, training=False)\n",
    "        noise = tf.random.normal(shape=next_actions.shape, mean=0, stddev=self.policy_noise)\n",
    "        noise = tf.clip_by_value(noise, -self.noise_clip, self.noise_clip)\n",
    "        next_actions = tf.clip_by_value(next_actions + noise, self.action_min, self.action_max)\n",
    "        next_state_actions = tf.concat((mb_next_states, next_actions), axis=1)\n",
    "\n",
    "        q1_next = self.target_critic_network1(next_state_actions, training=False)\n",
    "        q2_next = self.target_critic_network2(next_state_actions, training=False)\n",
    "        q_min_next = tf.minimum(q1_next, q2_next)\n",
    "        q_target = tf.stop_gradient(mb_rewards + self.discount_factor * (1 - mb_terminals) * q_min_next)\n",
    "\n",
    "        # Train critic 1 network.\n",
    "        with tf.GradientTape() as tape1:\n",
    "            q1_expected = self.critic_network1(mb_state_actions, training=True)\n",
    "            critic1_loss = tf.reduce_mean(tf.square(q1_expected - q_target))\n",
    "\n",
    "        critic1_grads = tape1.gradient(critic1_loss, self.critic_network1.trainable_variables)\n",
    "        self.critic1_optimizer.apply_gradients(zip(critic1_grads, self.critic_network1.trainable_variables))\n",
    "\n",
    "        # Train critic 2 network.\n",
    "        with tf.GradientTape() as tape2:\n",
    "            q2_expected = self.critic_network2(mb_state_actions, training=True)\n",
    "            critic2_loss = tf.reduce_mean(tf.square(q2_expected - q_target))\n",
    "\n",
    "        critic2_grads = tape2.gradient(critic2_loss, self.critic_network2.trainable_variables)\n",
    "        self.critic2_optimizer.apply_gradients(zip(critic2_grads, self.critic_network2.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def update_actor_network(self, minibatch: tf.Tensor):\n",
    "        \"\"\"Update the actor network.\"\"\"\n",
    "        mb_states = minibatch[:, self.state_slice]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            pred_actions = self.actor_network(mb_states, training=True)\n",
    "            pred_state_actions = tf.concat((mb_states, pred_actions), axis=1)\n",
    "            q_values = self.critic_network1(pred_state_actions, training=False)\n",
    "            actor_loss = -tf.reduce_mean(q_values)\n",
    "\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor_network.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\n",
    "\n",
    "    def soft_update_target_weights(self):\n",
    "        \"\"\"Soft update the target networks weights.\"\"\"\n",
    "        new_target_critic_network1_weights = [\n",
    "            self.tau * w_local + (1 - self.tau) * w_target\n",
    "            for w_local, w_target in zip(self.critic_network1.get_weights(), self.target_critic_network1.get_weights())\n",
    "        ]\n",
    "        self.target_critic_network1.set_weights(new_target_critic_network1_weights)\n",
    "\n",
    "        new_target_critic_network2_weights = [\n",
    "            self.tau * w_local + (1 - self.tau) * w_target\n",
    "            for w_local, w_target in zip(self.critic_network2.get_weights(), self.target_critic_network2.get_weights())\n",
    "        ]\n",
    "        self.target_critic_network2.set_weights(new_target_critic_network2_weights)\n",
    "\n",
    "        new_target_actor_weights = [\n",
    "            self.tau * w_local + (1 - self.tau) * w_target\n",
    "            for w_local, w_target in zip(self.actor_network.get_weights(), self.target_actor_network.get_weights())\n",
    "        ]\n",
    "        self.target_actor_network.set_weights(new_target_actor_weights)\n",
    "\n",
    "    def save_network_weights(self):\n",
    "        \"\"\"Save each network's weights.\"\"\"\n",
    "        self.critic_network1.save_weights(\"TD3 Models/critic_network1.weights.h5\")\n",
    "        self.critic_network2.save_weights(\"TD3 Models/critic_network2.weights.h5\")\n",
    "        self.actor_network.save_weights(\"TD3 Models/actor_network.weights.h5\")\n",
    "\n",
    "    def learn(self, n_episodes=1000, stop_after=10):\n",
    "\n",
    "        episode_rewards = []\n",
    "        step = 0 # Monitor time steps.\n",
    "\n",
    "        for n in range(n_episodes):\n",
    "            # Print episode number.\n",
    "            print(\"Episode:\", n + 1)\n",
    "\n",
    "            # Reset environment.\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            episode_reward = 0 # Monitor reward.\n",
    "\n",
    "            while True:\n",
    "                step += 1\n",
    "\n",
    "                # Select action.\n",
    "                action = self.select_action(state)\n",
    "\n",
    "                # Take step.\n",
    "                new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Store transition.\n",
    "                self.save_transition(state, action, reward, new_state, terminal)\n",
    "\n",
    "                # Update episode reward\n",
    "                episode_reward += reward\n",
    "\n",
    "                if self.buffer_fullness >= self.minibatch_size and self.buffer_fullness >= self.warm_up:\n",
    "                    # Sample minibatch.\n",
    "                    minibatch = self.sample_minibatch()\n",
    "\n",
    "                    # Update critic networks.\n",
    "                    self.update_critic_networks(minibatch)\n",
    "\n",
    "                    if step % self.policy_delay == 0:\n",
    "                        # Update actor network.\n",
    "                        self.update_actor_network(minibatch)\n",
    "\n",
    "                        # Update target weights.\n",
    "                        self.soft_update_target_weights()\n",
    "\n",
    "                if terminal or truncated:\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            # Print and save episode reward.\n",
    "            print(\"Episode reward:\", episode_reward)\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "            # Early stopping.\n",
    "            if all(ep_rwd >= self.max_reward for ep_rwd in episode_rewards[-stop_after:]):\n",
    "                break\n",
    "\n",
    "        self.save_network_weights()  # Save weights.\n",
    "        return episode_rewards"
   ],
   "id": "55a4942775beddc0",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:20:54.189990Z",
     "start_time": "2025-12-02T11:20:53.778903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "\n",
    "critic_network1 = Sequential([\n",
    "    Input(shape=(28,)),\n",
    "    Dense(256, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(256, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(1, activation=\"linear\", kernel_initializer=RandomUniform(-0.003, 0.003))  # No activation.\n",
    "])\n",
    "\n",
    "critic_network2 = Sequential([\n",
    "    Input(shape=(28,)),\n",
    "    Dense(256, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(256, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(1, activation=\"linear\", kernel_initializer=RandomUniform(-0.003, 0.003))  # No activation.\n",
    "])\n",
    "\n",
    "actor_network = Sequential([\n",
    "    Input(shape=(24,)),\n",
    "    Dense(256, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(256, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(4, activation=\"tanh\", kernel_initializer=RandomUniform(-0.003, 0.003))  # Tanh to map outputs to [-1, 1].\n",
    "])"
   ],
   "id": "afd39efc416cc6fb",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:22:18.019526Z",
     "start_time": "2025-12-02T11:22:17.271122Z"
    }
   },
   "cell_type": "code",
   "source": "env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=None)",
   "id": "aaea5a5726179447",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louie\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: Dimension check on the tf.reduce_mean functions in update actor and critics (maybe should use keepdims=True)",
   "id": "c0481bbfe7f2db0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T11:22:26.427991Z",
     "start_time": "2025-12-02T11:22:22.436208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test = TD3Agent(env, critic_network1, critic_network2, actor_network)\n",
    "ers = test.learn()"
   ],
   "id": "543296dd386280fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Episode reward: -93.891754\n",
      "Episode: 2\n",
      "Episode reward: -93.71252\n",
      "Episode: 3\n",
      "Episode reward: -118.1867\n",
      "Episode: 4\n",
      "Episode reward: -119.80537\n",
      "Episode: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_81348\\1808960135.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m()\u001B[39m\n\u001B[32m      1\u001B[39m test = TD3Agent(env, critic_network1, critic_network2, actor_network)\n\u001B[32m----> \u001B[39m\u001B[32m2\u001B[39m ers = test.learn()\n",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_81348\\2577572454.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, n_episodes, stop_after)\u001B[39m\n\u001B[32m    197\u001B[39m             \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[32m    198\u001B[39m                 step += \u001B[32m1\u001B[39m\n\u001B[32m    199\u001B[39m \n\u001B[32m    200\u001B[39m                 \u001B[38;5;66;03m# Select action.\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m201\u001B[39m                 action = self.select_action(state)\n\u001B[32m    202\u001B[39m \n\u001B[32m    203\u001B[39m                 \u001B[38;5;66;03m# Take step.\u001B[39;00m\n\u001B[32m    204\u001B[39m                 new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
      "\u001B[32m~\\AppData\\Local\\Temp\\ipykernel_81348\\2577572454.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, state)\u001B[39m\n\u001B[32m     76\u001B[39m         \u001B[33m\"\"\"Select an action at the given state.\"\"\"\u001B[39m\n\u001B[32m     77\u001B[39m         \u001B[38;5;66;03m# Input check.\u001B[39;00m\n\u001B[32m     78\u001B[39m         \u001B[38;5;28;01massert\u001B[39;00m state.ndim == \u001B[32m1\u001B[39m \u001B[38;5;28;01mand\u001B[39;00m state.shape[\u001B[32m0\u001B[39m] == self.state_size\n\u001B[32m     79\u001B[39m         \u001B[38;5;66;03m# Do forward pass.\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m80\u001B[39m         action = self.actor_network(np.expand_dims(state, axis=\u001B[32m0\u001B[39m), training=\u001B[38;5;28;01mFalse\u001B[39;00m).numpy()[\u001B[32m0\u001B[39m]\n\u001B[32m     81\u001B[39m         \u001B[38;5;66;03m# Add exploratory noise.\u001B[39;00m\n\u001B[32m     82\u001B[39m         noise = np.random.normal(\u001B[32m0\u001B[39m, self.exploratory_noise_std, self.action_size)\n\u001B[32m     83\u001B[39m         action += noise\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    120\u001B[39m             \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m             \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m    122\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    975\u001B[39m                     \u001B[33m\"layers will not see the mask.\"\u001B[39m\n\u001B[32m    976\u001B[39m                 )\n\u001B[32m    977\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    978\u001B[39m             \u001B[38;5;66;03m# Destroy call context if we created it\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m979\u001B[39m             self._maybe_reset_call_context()\n\u001B[32m    980\u001B[39m \n\u001B[32m    981\u001B[39m         \u001B[38;5;66;03m################################################\u001B[39;00m\n\u001B[32m    982\u001B[39m         \u001B[38;5;66;03m# 8. Add a node in the graph for symbolic calls.\u001B[39;00m\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    120\u001B[39m             \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m             \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m    122\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     55\u001B[39m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001B[32m     56\u001B[39m                 call_fn,\n\u001B[32m     57\u001B[39m                 object_name=(f\"{self.__class__.__name__}.call()\"),\n\u001B[32m     58\u001B[39m             )\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m call_fn(*args, **kwargs)\n\u001B[32m     60\u001B[39m \n\u001B[32m     61\u001B[39m         \u001B[38;5;66;03m# Plain flow.\u001B[39;00m\n\u001B[32m     62\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    213\u001B[39m                 new_e = e\n\u001B[32m    214\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m new_e.with_traceback(e.__traceback__) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    215\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    216\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m signature\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m bound_signature\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\models\\sequential.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, inputs, training, mask, **kwargs)\u001B[39m\n\u001B[32m    218\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m call(self, inputs, training=\u001B[38;5;28;01mNone\u001B[39;00m, mask=\u001B[38;5;28;01mNone\u001B[39;00m, **kwargs):\n\u001B[32m    219\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m self._functional:\n\u001B[32m--> \u001B[39m\u001B[32m220\u001B[39m             return self._functional.call(\n\u001B[32m    221\u001B[39m                 inputs, training=training, mask=mask, **kwargs\n\u001B[32m    222\u001B[39m             )\n\u001B[32m    223\u001B[39m \n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, inputs, training, mask, **kwargs)\u001B[39m\n\u001B[32m    179\u001B[39m             masks = tree.flatten(mask)\n\u001B[32m    180\u001B[39m             \u001B[38;5;28;01mfor\u001B[39;00m x, mask \u001B[38;5;28;01min\u001B[39;00m zip(inputs, masks):\n\u001B[32m    181\u001B[39m                 \u001B[38;5;28;01mif\u001B[39;00m mask \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    182\u001B[39m                     backend.set_keras_mask(x, mask)\n\u001B[32m--> \u001B[39m\u001B[32m183\u001B[39m         outputs = self._run_through_graph(\n\u001B[32m    184\u001B[39m             inputs,\n\u001B[32m    185\u001B[39m             operation_fn=lambda op: operation_fn(\n\u001B[32m    186\u001B[39m                 op, training=training, **kwargs\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\ops\\function.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, inputs, operation_fn, call_fn)\u001B[39m\n\u001B[32m    202\u001B[39m                 \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    203\u001B[39m                     \u001B[38;5;66;03m# Use NNX operation mapping\u001B[39;00m\n\u001B[32m    204\u001B[39m                     operation = self._get_operation_for_node(node)\n\u001B[32m    205\u001B[39m                     op = operation_fn(operation)\n\u001B[32m--> \u001B[39m\u001B[32m206\u001B[39m                     outputs = op(*args, **kwargs)\n\u001B[32m    207\u001B[39m \n\u001B[32m    208\u001B[39m                 \u001B[38;5;66;03m# Update tensor_dict.\u001B[39;00m\n\u001B[32m    209\u001B[39m                 \u001B[38;5;28;01mfor\u001B[39;00m x, y \u001B[38;5;28;01min\u001B[39;00m zip(node.outputs, tree.flatten(outputs)):\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\models\\functional.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    640\u001B[39m                 \u001B[38;5;28;01mand\u001B[39;00m value \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    641\u001B[39m             ):\n\u001B[32m    642\u001B[39m                 kwargs[name] = value\n\u001B[32m    643\u001B[39m \n\u001B[32m--> \u001B[39m\u001B[32m644\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m operation(*args, **kwargs)\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    120\u001B[39m             \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m             \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m    122\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\layers\\layer.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m    975\u001B[39m                     \u001B[33m\"layers will not see the mask.\"\u001B[39m\n\u001B[32m    976\u001B[39m                 )\n\u001B[32m    977\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    978\u001B[39m             \u001B[38;5;66;03m# Destroy call context if we created it\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m979\u001B[39m             self._maybe_reset_call_context()\n\u001B[32m    980\u001B[39m \n\u001B[32m    981\u001B[39m         \u001B[38;5;66;03m################################################\u001B[39;00m\n\u001B[32m    982\u001B[39m         \u001B[38;5;66;03m# 8. Add a node in the graph for symbolic calls.\u001B[39;00m\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    120\u001B[39m             \u001B[38;5;66;03m# To get the full stack trace, call:\u001B[39;00m\n\u001B[32m    121\u001B[39m             \u001B[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001B[39;00m\n\u001B[32m    122\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    123\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m124\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\ops\\operation.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m     55\u001B[39m             call_fn = traceback_utils.inject_argument_info_in_traceback(\n\u001B[32m     56\u001B[39m                 call_fn,\n\u001B[32m     57\u001B[39m                 object_name=(f\"{self.__class__.__name__}.call()\"),\n\u001B[32m     58\u001B[39m             )\n\u001B[32m---> \u001B[39m\u001B[32m59\u001B[39m             \u001B[38;5;28;01mreturn\u001B[39;00m call_fn(*args, **kwargs)\n\u001B[32m     60\u001B[39m \n\u001B[32m     61\u001B[39m         \u001B[38;5;66;03m# Plain flow.\u001B[39;00m\n\u001B[32m     62\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m any_symbolic_tensors(args, kwargs):\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    213\u001B[39m                 new_e = e\n\u001B[32m    214\u001B[39m             \u001B[38;5;28;01mraise\u001B[39;00m new_e.with_traceback(e.__traceback__) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    215\u001B[39m         \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    216\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m signature\n\u001B[32m--> \u001B[39m\u001B[32m217\u001B[39m             \u001B[38;5;28;01mdel\u001B[39;00m bound_signature\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(self, inputs, training)\u001B[39m\n\u001B[32m    186\u001B[39m     \u001B[38;5;28;01mdef\u001B[39;00m call(self, inputs, training=\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m187\u001B[39m         x = ops.matmul(inputs, self.kernel)\n\u001B[32m    188\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m self.bias \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m    189\u001B[39m             x = ops.add(x, self.bias)\n\u001B[32m    190\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m self.activation \u001B[38;5;28;01mis\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\ops\\numpy.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(x1, x2)\u001B[39m\n\u001B[32m   4603\u001B[39m         Output tensor, matrix product of the inputs.\n\u001B[32m   4604\u001B[39m     \"\"\"\n\u001B[32m   4605\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m any_symbolic_tensors((x1, x2)):\n\u001B[32m   4606\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m Matmul().symbolic_call(x1, x2)\n\u001B[32m-> \u001B[39m\u001B[32m4607\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m backend.numpy.matmul(x1, x2)\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\numpy.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(x1, x2)\u001B[39m\n\u001B[32m    616\u001B[39m         output.set_shape(output_shape)\n\u001B[32m    617\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[32m    618\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m    619\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m x1_shape.rank == \u001B[32m2\u001B[39m \u001B[38;5;28;01mand\u001B[39;00m x2_shape.rank == \u001B[32m2\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m620\u001B[39m             output = tf.matmul(x1, x2, output_type=output_type)\n\u001B[32m    621\u001B[39m         \u001B[38;5;28;01melif\u001B[39;00m x2_shape.rank == \u001B[32m1\u001B[39m:\n\u001B[32m    622\u001B[39m             output = tf.tensordot(x1, x2, axes=\u001B[32m1\u001B[39m)\n\u001B[32m    623\u001B[39m         \u001B[38;5;28;01melif\u001B[39;00m x1_shape.rank == \u001B[32m1\u001B[39m:\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\weak_tensor_ops.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    140\u001B[39m   \u001B[38;5;28;01mdef\u001B[39;00m wrapper(*args, **kwargs):\n\u001B[32m    141\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28;01mnot\u001B[39;00m ops.is_auto_dtype_conversion_enabled():\n\u001B[32m--> \u001B[39m\u001B[32m142\u001B[39m       \u001B[38;5;28;01mreturn\u001B[39;00m op(*args, **kwargs)\n\u001B[32m    143\u001B[39m     bound_arguments = signature.bind(*args, **kwargs)\n\u001B[32m    144\u001B[39m     bound_arguments.apply_defaults()\n\u001B[32m    145\u001B[39m     bound_kwargs = bound_arguments.arguments\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m    151\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m Exception \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m    152\u001B[39m       filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001B[32m    153\u001B[39m       \u001B[38;5;28;01mraise\u001B[39;00m e.with_traceback(filtered_tb) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m    154\u001B[39m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m155\u001B[39m       \u001B[38;5;28;01mdel\u001B[39;00m filtered_tb\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(*args, **kwargs)\u001B[39m\n\u001B[32m   1261\u001B[39m \n\u001B[32m   1262\u001B[39m       \u001B[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001B[39;00m\n\u001B[32m   1263\u001B[39m       \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   1264\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m dispatch_target(*args, **kwargs)\n\u001B[32m-> \u001B[39m\u001B[32m1265\u001B[39m       \u001B[38;5;28;01mexcept\u001B[39;00m (TypeError, ValueError):\n\u001B[32m   1266\u001B[39m         \u001B[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001B[39;00m\n\u001B[32m   1267\u001B[39m         \u001B[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001B[39;00m\n\u001B[32m   1268\u001B[39m         result = dispatch(op_dispatch_handler, args, kwargs)\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(a, b, transpose_a, transpose_b, adjoint_a, adjoint_b, a_is_sparse, b_is_sparse, output_type, grad_a, grad_b, name)\u001B[39m\n\u001B[32m   3696\u001B[39m             grad_y=grad_b,\n\u001B[32m   3697\u001B[39m             name=name,\n\u001B[32m   3698\u001B[39m         )\n\u001B[32m   3699\u001B[39m       \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m3700\u001B[39m         return gen_math_ops.mat_mul(\n\u001B[32m   3701\u001B[39m             a,\n\u001B[32m   3702\u001B[39m             b,\n\u001B[32m   3703\u001B[39m             transpose_a=transpose_a,\n",
      "\u001B[32m~\\OneDrive\\Documents\\.venv\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_math_ops.py\u001B[39m in \u001B[36m?\u001B[39m\u001B[34m(a, b, transpose_a, transpose_b, grad_a, grad_b, name)\u001B[39m\n\u001B[32m   6232\u001B[39m         transpose_b, \u001B[33m\"grad_a\"\u001B[39m, grad_a, \u001B[33m\"grad_b\"\u001B[39m, grad_b)\n\u001B[32m   6233\u001B[39m       \u001B[38;5;28;01mreturn\u001B[39;00m _result\n\u001B[32m   6234\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m _core._NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[32m   6235\u001B[39m       _ops.raise_from_not_ok_status(e, name)\n\u001B[32m-> \u001B[39m\u001B[32m6236\u001B[39m     \u001B[38;5;28;01mexcept\u001B[39;00m _core._FallbackException:\n\u001B[32m   6237\u001B[39m       \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[32m   6238\u001B[39m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m   6239\u001B[39m       return mat_mul_eager_fallback(\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 10
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
