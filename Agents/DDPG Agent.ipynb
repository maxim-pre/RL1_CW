{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-29T12:34:03.375234Z",
     "start_time": "2025-11-29T12:33:56.028945Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras.models import clone_model\n",
    "import gymnasium as gym"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T12:42:29.606804Z",
     "start_time": "2025-11-29T12:42:29.585370Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class DDPGAgent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env,\n",
    "                 critic_network,\n",
    "                 actor_network,\n",
    "                 critic_learning_rate=1e-3,\n",
    "                 actor_learning_rate=1e-3,\n",
    "                 discount_factor=0.99,\n",
    "                 minibatch_size=100,\n",
    "                 tau=0.005,\n",
    "                 exploratory_noise_std=0.3,\n",
    "                 warm_up=1000,\n",
    "                 max_buffer_length=1_000_000):\n",
    "        self.env = env\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.shape[0]\n",
    "        self.action_min, self.action_max = env.action_space.low, env.action_space.high\n",
    "\n",
    "        # Initialize critic network and target critic network.\n",
    "        self.critic_network = clone_model(critic_network)\n",
    "        self.critic_network.set_weights(critic_network.get_weights())\n",
    "\n",
    "        self.target_critic_network = clone_model(critic_network)\n",
    "        self.target_critic_network.set_weights(critic_network.get_weights())\n",
    "\n",
    "        # Initialize actor network and target actor network.\n",
    "        self.actor_network = clone_model(actor_network)\n",
    "        self.actor_network.set_weights(actor_network.get_weights())\n",
    "\n",
    "        self.target_actor_network = clone_model(actor_network)\n",
    "        self.target_actor_network.set_weights(actor_network.get_weights())\n",
    "\n",
    "        # Initialize optimizers.\n",
    "        self.critic_optimizer = Adam(learning_rate=critic_learning_rate)\n",
    "        self.actor_optimizer = Adam(learning_rate=actor_learning_rate)\n",
    "\n",
    "        # Initialize hyperparameters.\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.exploratory_noise_std = exploratory_noise_std\n",
    "        self.warm_up = warm_up\n",
    "\n",
    "        # Initialize buffer.\n",
    "        self.buffer_width = 2 * self.state_size + self.action_size + 2\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.replay_buffer = np.zeros((self.max_buffer_length, self.buffer_width), dtype=np.float32)\n",
    "        self.buffer_write_idx = 0\n",
    "        self.buffer_fullness = 0\n",
    "\n",
    "        # Initialize minibatch slicers.\n",
    "        self.state_slice = slice(0, self.state_size)\n",
    "        self.state_action_slice = slice(0, self.state_size + self.action_size)\n",
    "        self.reward_slice = slice(self.state_size + self.action_size, self.state_size + self.action_size + 1)\n",
    "        self.next_state_slice = slice(self.state_size + self.action_size + 1, 2 * self.state_size + self.action_size + 1)\n",
    "        self.reward_slice = slice(2 * self.state_size + self.action_size + 1, 2 * self.state_size + self.action_size + 2)\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action at the given state.\"\"\"\n",
    "        # Input check.\n",
    "        assert state.ndim == 1 and state.shape[0] == self.state_size\n",
    "        # Do forward pass.\n",
    "        action = self.actor_network(np.expand_dims(state, axis=0), training=False).numpy()[0]\n",
    "        # Add exploratory noise.\n",
    "        noise = np.random.normal(0, self.exploratory_noise_std, self.action_size)\n",
    "        action += noise\n",
    "        action = np.clip(action, self.action_min, self.action_max)\n",
    "        return action\n",
    "\n",
    "    def save_transition(self, state: np.ndarray, action: np.ndarray, reward: float, new_state: np.ndarray, terminal: bool):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        # Input check.\n",
    "        assert state.ndim == 1 and state.shape[0] == self.state_size\n",
    "        assert action.ndim == 1 and action.shape[0] == self.action_size\n",
    "        assert new_state.ndim == 1 and new_state.shape[0] == self.state_size\n",
    "\n",
    "        # Save transition.\n",
    "        transition = np.concatenate((state, action, [reward], new_state, [1.0 if terminal else 0.0]), dtype=np.float32)\n",
    "        self.replay_buffer[self.buffer_write_idx] = transition\n",
    "\n",
    "        # Update write index and fullness.\n",
    "        self.buffer_write_idx = (self.buffer_write_idx + 1) % self.max_buffer_length\n",
    "        self.buffer_fullness = min(self.buffer_fullness + 1, self.max_buffer_length)\n",
    "\n",
    "    def sample_minibatch(self) -> tf.Tensor:\n",
    "        \"\"\"Sample a minibatch from the replay buffer.\"\"\"\n",
    "        indices = np.random.choice(self.buffer_fullness, size=self.minibatch_size, replace=False)\n",
    "        minibatch = self.replay_buffer[indices]\n",
    "        return tf.convert_to_tensor(minibatch, dtype=tf.float32)\n",
    "\n",
    "    @tf.function\n",
    "    def update_critic_network(self, minibatch: tf.Tensor):\n",
    "        \"\"\"Update the critic network.\"\"\"\n",
    "        mb_state_actions = minibatch[:, self.state_action_slice]\n",
    "        mb_rewards = minibatch[:, self.reward_slice]\n",
    "        mb_next_states = minibatch[:, self.next_state_slice]\n",
    "        mb_terminals = minibatch[:, self.reward_slice]\n",
    "\n",
    "        next_actions = self.target_actor_network(mb_next_states, training=False)\n",
    "        next_state_actions = tf.concat((mb_next_states, next_actions), axis=1)\n",
    "        q_next = self.target_critic_network(next_state_actions, training=False)\n",
    "        q_target = tf.stop_gradient(mb_rewards + self.discount_factor * (1.0 - mb_terminals) * q_next)\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_expected = self.critic_network(mb_state_actions, training=True)\n",
    "            critic_loss = tf.reduce_mean(tf.square(q_target - q_expected))\n",
    "\n",
    "        critic_grads = tape.gradient(critic_loss, self.critic_network.trainable_variables)\n",
    "        self.critic_optimizer.apply_gradients(zip(critic_grads, self.critic_network.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def update_actor_network(self, minibatch: tf.Tensor):\n",
    "        \"\"\"Update the actor network.\"\"\"\n",
    "        mb_states = minibatch[:, self.state_slice]\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            raw_actions = self.actor_network(mb_states, training=True)\n",
    "            raw_state_actions = tf.concat((mb_states, raw_actions), axis=1)\n",
    "            q_values = self.critic_network(raw_state_actions, training=False)\n",
    "            actor_loss = -tf.reduce_mean(q_values)\n",
    "\n",
    "        actor_grads = tape.gradient(actor_loss, self.actor_network.trainable_variables)\n",
    "        self.actor_optimizer.apply_gradients(zip(actor_grads, self.actor_network.trainable_variables))\n",
    "\n",
    "    def soft_update_target_weights(self):\n",
    "        \"\"\"Soft update the target networks weights.\"\"\"\n",
    "        new_target_critic_weights = [\n",
    "            self.tau * w_local + (1 - self.tau) * w_target\n",
    "            for w_local, w_target in zip(self.critic_network.get_weights(), self.target_critic_network.get_weights())\n",
    "        ]\n",
    "        self.target_critic_network.set_weights(new_target_critic_weights)\n",
    "\n",
    "        new_target_actor_weights = [\n",
    "            self.tau * w_local + (1 - self.tau) * w_target\n",
    "            for w_local, w_target in zip(self.actor_network.get_weights(), self.target_actor_network.get_weights())\n",
    "        ]\n",
    "        self.target_actor_network.set_weights(new_target_actor_weights)\n",
    "\n",
    "    def save_network_weights(self):\n",
    "        \"\"\"Save each network's weights.\"\"\"\n",
    "        self.critic_network.save_weights(\"DDPG Models/critic_network.weights.h5\")\n",
    "        self.actor_network.save_weights(\"DDPG Models/actor_network.weights.h5\")\n",
    "\n",
    "    def learn(self, n_episodes=1000):\n",
    "\n",
    "        episode_rewards = []\n",
    "\n",
    "        for n in range(n_episodes):\n",
    "            # Print episode number.\n",
    "            print(\"Episode:\", n + 1)\n",
    "\n",
    "            # Reset environment.\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            # Monitor reward\n",
    "            episode_reward = 0\n",
    "\n",
    "            while True:\n",
    "                # Select action.\n",
    "                action = self.select_action(state)\n",
    "\n",
    "                # Take step.\n",
    "                new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Store transition.\n",
    "                self.save_transition(state, action, reward, new_state, terminal)\n",
    "\n",
    "                # Update episode reward\n",
    "                episode_reward += reward\n",
    "\n",
    "                if self.buffer_fullness >= self.minibatch_size and self.buffer_fullness >= self.warm_up:\n",
    "                    # Sample minibatch.\n",
    "                    minibatch = self.sample_minibatch()\n",
    "\n",
    "                    # Update critic network.\n",
    "                    self.update_critic_network(minibatch)\n",
    "\n",
    "                    # Update actor network.\n",
    "                    self.update_actor_network(minibatch)\n",
    "\n",
    "                    # Update target weights.\n",
    "                    self.soft_update_target_weights()\n",
    "\n",
    "                if terminal or truncated:\n",
    "                    break\n",
    "\n",
    "                state = new_state  # Update state.\n",
    "\n",
    "            # Print and save episode reward.\n",
    "            print(\"Episode reward:\", episode_reward)\n",
    "            episode_rewards.append(episode_reward)\n",
    "\n",
    "        self.save_network_weights()  # Save weights.\n",
    "        return episode_rewards"
   ],
   "id": "55a4942775beddc0",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T12:34:05.614845Z",
     "start_time": "2025-11-29T12:34:05.445187Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Input, Dense\n",
    "from keras.initializers import RandomUniform\n",
    "\n",
    "\n",
    "critic_network = Sequential([\n",
    "    Input(shape=(28,)),\n",
    "    Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(1, activation=\"linear\", kernel_initializer=RandomUniform(-0.003, 0.003))  # No activation.\n",
    "])\n",
    "\n",
    "actor_network = Sequential([\n",
    "    Input(shape=(24,)),\n",
    "    Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(64, activation=\"relu\", kernel_initializer=\"he_uniform\"),\n",
    "    Dense(4, activation=\"tanh\", kernel_initializer=RandomUniform(-0.003, 0.003))  # Tanh to map outputs to [-1, 1].\n",
    "])"
   ],
   "id": "afd39efc416cc6fb",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T12:34:06.477957Z",
     "start_time": "2025-11-29T12:34:06.233002Z"
    }
   },
   "cell_type": "code",
   "source": "env = gym.make(\"BipedalWalker-v3\", hardcore=False, render_mode=None)",
   "id": "aaea5a5726179447",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louie\\OneDrive - University of Bath\\.venv\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-29T12:34:43.233164Z",
     "start_time": "2025-11-29T12:34:07.814462Z"
    }
   },
   "cell_type": "code",
   "source": [
    "test = DDPGAgent(env, critic_network, actor_network)\n",
    "test.learn()"
   ],
   "id": "543296dd386280fa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n",
      "Buffer full, updating critic network\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
