{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-04T11:29:19.911099Z",
     "start_time": "2025-12-04T11:29:19.902228Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TD3Agent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env_id,\n",
    "                 env_hardcore,\n",
    "                 critic_network1,\n",
    "                 critic_network2,\n",
    "                 actor_network,\n",
    "                 critic_learning_rate=1e-3,\n",
    "                 actor_learning_rate=1e-4,\n",
    "                 discount_factor=0.99,\n",
    "                 minibatch_size=256,\n",
    "                 tau=0.005,\n",
    "                 exploratory_noise=0.1,\n",
    "                 exploratory_noise_clip=0.3,\n",
    "                 policy_noise=0.1,\n",
    "                 policy_noise_clip=0.5,\n",
    "                 policy_delay=2,\n",
    "                 warm_up=10_000,\n",
    "                 gradient_clip=0.0,\n",
    "                 updates_per_step=1,\n",
    "                 max_buffer_length=250_000):\n",
    "        # CPU or GPU?\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Environment.\n",
    "        self.env_id = env_id\n",
    "        self.env_hardcore = env_hardcore\n",
    "        self.env = gym.make(id=env_id, hardcore=env_hardcore, render_mode=None)\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.shape[0]\n",
    "        self.action_min = torch.tensor(self.env.action_space.low, device=self.device)\n",
    "        self.action_max = torch.tensor(self.env.action_space.high, device=self.device)\n",
    "        self.max_reward = self.env.spec.reward_threshold\n",
    "\n",
    "        # Initialize critic network 1 and target critic network 1.\n",
    "        self.critic_network1 = copy.deepcopy(critic_network1)\n",
    "        self.target_critic_network1 = copy.deepcopy(critic_network1)\n",
    "\n",
    "        # Initialize critic network 2 and target critic network 2.\n",
    "        self.critic_network2 = copy.deepcopy(critic_network2)\n",
    "        self.target_critic_network2 = copy.deepcopy(critic_network2)\n",
    "\n",
    "        # Initialize actor network and target actor network.\n",
    "        self.actor_network = copy.deepcopy(actor_network)\n",
    "        self.target_actor_network = copy.deepcopy(actor_network)\n",
    "\n",
    "        # Move networks to correct device.\n",
    "        self.actor_network.to(self.device)\n",
    "        self.critic_network1.to(self.device)\n",
    "        self.critic_network2.to(self.device)\n",
    "        self.target_actor_network.to(self.device)\n",
    "        self.target_critic_network1.to(self.device)\n",
    "        self.target_critic_network2.to(self.device)\n",
    "\n",
    "        # Initialize optimizers.\n",
    "        self.critic1_optimizer = optim.Adam(self.critic_network1.parameters(), lr=critic_learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic_network2.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=actor_learning_rate)\n",
    "\n",
    "        # Initialize hyperparameters.\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.warm_up = warm_up\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.exploratory_noise = exploratory_noise\n",
    "        self.exploratory_noise_clip = exploratory_noise_clip\n",
    "        self.policy_noise = policy_noise\n",
    "        self.policy_noise_clip = policy_noise_clip\n",
    "        self.policy_delay = policy_delay\n",
    "        self.updates_per_step = updates_per_step\n",
    "\n",
    "        # Initialize buffer.\n",
    "        self.buffer_width = 2 * self.state_size + self.action_size + 2\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.buffer_write_idx = 0\n",
    "        self.buffer_fullness = 0\n",
    "        self.buffer_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                         dtype=torch.float32,\n",
    "                                         device=self.device)\n",
    "        self.buffer_actions = torch.zeros((self.max_buffer_length, self.action_size),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_rewards = torch.zeros((self.max_buffer_length, 1),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_next_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                              dtype=torch.float32,\n",
    "                                              device=self.device)\n",
    "        self.buffer_terminals = torch.zeros((self.max_buffer_length, 1),\n",
    "                                            dtype=torch.float32,\n",
    "                                            device=self.device)\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action at the given state.\"\"\"\n",
    "        self.actor_network.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass.\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            action_tensor = self.actor_network(state_tensor).squeeze(0)\n",
    "            # Add exploratory noise.\n",
    "            noise = torch.randn(self.action_size, device=self.device) * self.exploratory_noise\n",
    "            noise_clipped = torch.clamp(noise, min=-self.exploratory_noise_clip, max=self.exploratory_noise_clip)\n",
    "            action_tensor = action_tensor + noise_clipped\n",
    "            # Clip action.\n",
    "            action_tensor = torch.clamp(action_tensor, min=self.action_min, max=self.action_max)\n",
    "            return action_tensor.cpu().numpy()\n",
    "\n",
    "    def save_transition(self, state: np.ndarray, action: np.ndarray, reward: float, new_state: np.ndarray, terminal: bool):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        # Save transition.\n",
    "        self.buffer_states[self.buffer_write_idx] = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_actions[self.buffer_write_idx] = torch.tensor(action, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_rewards[self.buffer_write_idx] = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        self.buffer_next_states[self.buffer_write_idx] = torch.tensor(new_state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_terminals[self.buffer_write_idx] = torch.tensor([1.0 if terminal else 0.0],\n",
    "                                                                    dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.buffer_write_idx = (self.buffer_write_idx + 1) % self.max_buffer_length\n",
    "        self.buffer_fullness = min(self.buffer_fullness + 1, self.max_buffer_length)\n",
    "\n",
    "    def sample_minibatch(self):\n",
    "        \"\"\"Sample a minibatch from the replay buffer.\"\"\"\n",
    "        indices = torch.randint(0, self.buffer_fullness, (self.minibatch_size,), device=self.device)\n",
    "\n",
    "        mb_states = self.buffer_states[indices]\n",
    "        mb_actions = self.buffer_actions[indices]\n",
    "        mb_rewards = self.buffer_rewards[indices]\n",
    "        mb_next_states = self.buffer_next_states[indices]\n",
    "        mb_terminals = self.buffer_terminals[indices]\n",
    "\n",
    "        return mb_states, mb_actions, mb_rewards, mb_next_states, mb_terminals\n",
    "\n",
    "    def update_critic_networks(self, minibatch: torch.Tensor):\n",
    "        \"\"\"Update critic networks\"\"\"\n",
    "        mb_states, mb_actions, mb_rewards, mb_next_states, mb_terminals = minibatch\n",
    "        mb_state_actions = torch.cat([mb_states, mb_actions], dim=1)\n",
    "\n",
    "        self.target_actor_network.eval()\n",
    "        self.target_critic_network1.eval()\n",
    "        self.target_critic_network2.eval()\n",
    "        self.critic_network1.train()\n",
    "        self.critic_network2.train()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor_network(mb_next_states)\n",
    "            noise = torch.randn(next_actions.shape, device=self.device) * self.policy_noise\n",
    "            noise = torch.clamp(noise, min=-self.policy_noise_clip, max=self.policy_noise_clip)\n",
    "            next_actions = torch.clamp(next_actions + noise, min=self.action_min, max=self.action_max)\n",
    "            next_state_actions = torch.cat((mb_next_states, next_actions), dim=1)\n",
    "\n",
    "            q1_next = self.target_critic_network1(next_state_actions)\n",
    "            q2_next = self.target_critic_network2(next_state_actions)\n",
    "            q_min_next = torch.min(q1_next, q2_next)\n",
    "            q_target = mb_rewards + self.discount_factor * (1 - mb_terminals) * q_min_next\n",
    "\n",
    "        # Critic network 1 update.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network1(mb_state_actions)\n",
    "            critic1_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic_network1.parameters(), self.gradient_clip)\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        # Critic network 2 update.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network2(mb_state_actions)\n",
    "            critic2_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        if self.gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.critic_network2.parameters(), self.gradient_clip)\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "    def update_actor_network(self, minibatch: torch.Tensor):\n",
    "        \"\"\"Update the actor network.\"\"\"\n",
    "        mb_states, *_ = minibatch\n",
    "\n",
    "        self.actor_network.train()\n",
    "        self.critic_network1.eval()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            raw_actions = self.actor_network(mb_states)\n",
    "            raw_state_actions = torch.cat((mb_states, raw_actions), dim=1)\n",
    "            actor_loss = -self.critic_network1(raw_state_actions).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def soft_update_target_weights(self):\n",
    "        \"\"\"Soft update the target networks weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for w_target, w_local in zip(self.target_actor_network.parameters(), self.actor_network.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "            for w_target, w_local in zip(self.target_critic_network1.parameters(), self.critic_network1.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "            for w_target, w_local in zip(self.target_critic_network2.parameters(), self.critic_network2.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "    def save_outputs(self, episode_rewards, episode_step_counts, episode_run_times):\n",
    "        \"\"\"Save each network's weights and episode rewards.\"\"\"\n",
    "        folder = \"TD3 Outputs (hardcore)\" if self.env_hardcore else \"TD3 Outputs (normal)\"\n",
    "        torch.save(self.critic_network1.state_dict(), folder + \"/critic_network1.pth\")\n",
    "        torch.save(self.critic_network2.state_dict(), folder + \"/critic_network2.pth\")\n",
    "        torch.save(self.actor_network.state_dict(), folder + \"/actor_network.pth\")\n",
    "        with open(folder + \"/episode_rewards.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_rewards, fp)\n",
    "        with open(folder + \"/episode_step_counts.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_step_counts, fp)\n",
    "        with open(folder + \"/episode_run_times.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_run_times, fp)\n",
    "\n",
    "    def show_test_episode(self):\n",
    "        \"\"\"Do a visual test run.\"\"\"\n",
    "        print(\"\\n========TEST RUN========\")\n",
    "        test_env = gym.make(id=self.env_id, hardcore=self.env_hardcore, render_mode=\"human\")\n",
    "        s, _ = test_env.reset()\n",
    "        test_episode_reward = 0\n",
    "        test_episode_step_count = 0\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            a = self.select_action(s)\n",
    "            s_, r, terminated, truncated, _ = test_env.step(a)\n",
    "            test_episode_reward += r\n",
    "            test_episode_step_count += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            s = s_\n",
    "\n",
    "        test_episode_end_time = time.time()\n",
    "        test_episode_run_time = test_episode_end_time - test_start_time\n",
    "        test_env.close()\n",
    "\n",
    "        print(f\"Reward: {test_episode_reward:.2f} - Step Count: {test_episode_step_count} - Run Time: {test_episode_run_time:.2f}s\\n\")\n",
    "\n",
    "    def learn(self, n_episodes=2000, display_every=50, stop_after=None):\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_step_counts = []\n",
    "        episode_run_times = []\n",
    "\n",
    "        for n in range(n_episodes):\n",
    "            # Print episode number.\n",
    "            print(f\"Running Episode {n + 1}...\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Reset environment.\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_step_count = 0\n",
    "\n",
    "            while True:\n",
    "                # Select action and take step.\n",
    "                action = self.select_action(state)\n",
    "                new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Store transition.\n",
    "                self.save_transition(state, action, reward, new_state, terminal)\n",
    "\n",
    "                # Update episode reward\n",
    "                episode_step_count += 1\n",
    "                episode_reward += reward\n",
    "\n",
    "                if self.buffer_fullness >= self.minibatch_size and self.buffer_fullness >= self.warm_up:\n",
    "\n",
    "                    for _ in range(self.updates_per_step):\n",
    "\n",
    "                        minibatch = self.sample_minibatch()\n",
    "                        self.update_critic_networks(minibatch)\n",
    "\n",
    "                        if episode_step_count % self.policy_delay == 0:\n",
    "\n",
    "                            self.update_actor_network(minibatch)\n",
    "                            self.soft_update_target_weights()\n",
    "\n",
    "                if terminal or truncated:\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            end_time = time.time()\n",
    "            episode_run_time = end_time - start_time\n",
    "\n",
    "            # Print and save episode reward.\n",
    "            print(f\"Reward: {episode_reward:.2f} - Step Count: {episode_step_count} - Run Time: {episode_run_time:.2f}s\")\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_step_counts.append(episode_step_count)\n",
    "            episode_run_times.append(episode_run_time)\n",
    "\n",
    "            # Early stopping.\n",
    "            if stop_after is not None and all(ep_rew >= self.max_reward for ep_rew in episode_rewards[-stop_after:]):\n",
    "                break\n",
    "\n",
    "            if n % display_every == 0:\n",
    "                self.show_test_episode()\n",
    "\n",
    "        self.save_outputs(episode_rewards, episode_step_counts, episode_run_times)"
   ],
   "id": "e53644611ff62c1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:29:28.205944Z",
     "start_time": "2025-12-04T11:29:28.200115Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "857045117c0b1f95",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:29:28.909611Z",
     "start_time": "2025-12-04T11:29:28.901573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(24, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "ae27c0fc1754edad",
   "outputs": [],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T11:29:32.891953Z",
     "start_time": "2025-12-04T11:29:29.979199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train the robot.\n",
    "\n",
    "actor = Actor()\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = TD3Agent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=actor)\n",
    "\n",
    "agent.learn(stop_after=1)"
   ],
   "id": "b87e24a317c9ed6e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1\n",
      "Episode reward: -93.66564\n",
      "Episode: 2\n",
      "Episode reward: -93.320244\n",
      "Episode: 3\n",
      "Episode reward: -93.28207\n",
      "Episode: 4\n",
      "Episode reward: -92.92917\n",
      "Episode: 5\n",
      "Episode reward: -93.10777\n",
      "Episode: 6\n",
      "Episode reward: -94.00552\n",
      "Episode: 7\n",
      "Episode reward: -92.5566\n",
      "Episode: 8\n",
      "Episode reward: -92.56265\n",
      "Episode: 9\n",
      "Episode reward: -92.022194\n",
      "Episode: 10\n",
      "Episode reward: -92.90067\n",
      "Episode: 11\n",
      "Episode reward: -103.77015\n",
      "Episode: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[56]\u001B[39m\u001B[32m, line 8\u001B[39m\n\u001B[32m      5\u001B[39m critic2 = Critic()\n\u001B[32m      7\u001B[39m agent = TD3Agent(env, critic1, critic2, actor)\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 222\u001B[39m, in \u001B[36mTD3Agent.learn\u001B[39m\u001B[34m(self, n_episodes, stop_after)\u001B[39m\n\u001B[32m    219\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m.buffer_fullness >= \u001B[38;5;28mself\u001B[39m.minibatch_size \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m.buffer_fullness >= \u001B[38;5;28mself\u001B[39m.warm_up:\n\u001B[32m    221\u001B[39m     minibatch = \u001B[38;5;28mself\u001B[39m.sample_minibatch()\n\u001B[32m--> \u001B[39m\u001B[32m222\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mupdate_critic_networks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mminibatch\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    224\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m step % \u001B[38;5;28mself\u001B[39m.policy_delay == \u001B[32m0\u001B[39m:\n\u001B[32m    226\u001B[39m         \u001B[38;5;28mself\u001B[39m.update_actor_network(minibatch)\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[53]\u001B[39m\u001B[32m, line 141\u001B[39m, in \u001B[36mTD3Agent.update_critic_networks\u001B[39m\u001B[34m(self, minibatch)\u001B[39m\n\u001B[32m    138\u001B[39m     critic1_loss = torch.mean((q_target - q_expected) ** \u001B[32m2\u001B[39m)\n\u001B[32m    140\u001B[39m \u001B[38;5;28mself\u001B[39m.critic1_optimizer.zero_grad()\n\u001B[32m--> \u001B[39m\u001B[32m141\u001B[39m \u001B[43mcritic1_loss\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[38;5;28mself\u001B[39m.critic1_optimizer.step()\n\u001B[32m    144\u001B[39m \u001B[38;5;66;03m# Critic network 2 update.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Python Environments\\RL1_CW\\Lib\\site-packages\\torch\\_tensor.py:625\u001B[39m, in \u001B[36mTensor.backward\u001B[39m\u001B[34m(self, gradient, retain_graph, create_graph, inputs)\u001B[39m\n\u001B[32m    615\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[32m    616\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[32m    617\u001B[39m         Tensor.backward,\n\u001B[32m    618\u001B[39m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[32m   (...)\u001B[39m\u001B[32m    623\u001B[39m         inputs=inputs,\n\u001B[32m    624\u001B[39m     )\n\u001B[32m--> \u001B[39m\u001B[32m625\u001B[39m \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mautograd\u001B[49m\u001B[43m.\u001B[49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    626\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m=\u001B[49m\u001B[43minputs\u001B[49m\n\u001B[32m    627\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Python Environments\\RL1_CW\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001B[39m, in \u001B[36mbackward\u001B[39m\u001B[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[39m\n\u001B[32m    349\u001B[39m     retain_graph = create_graph\n\u001B[32m    351\u001B[39m \u001B[38;5;66;03m# The reason we repeat the same comment below is that\u001B[39;00m\n\u001B[32m    352\u001B[39m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[32m    353\u001B[39m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m354\u001B[39m \u001B[43m_engine_run_backward\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    355\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    356\u001B[39m \u001B[43m    \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    357\u001B[39m \u001B[43m    \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    358\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    359\u001B[39m \u001B[43m    \u001B[49m\u001B[43minputs_tuple\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    360\u001B[39m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    361\u001B[39m \u001B[43m    \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    362\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\Python Environments\\RL1_CW\\Lib\\site-packages\\torch\\autograd\\graph.py:841\u001B[39m, in \u001B[36m_engine_run_backward\u001B[39m\u001B[34m(t_outputs, *args, **kwargs)\u001B[39m\n\u001B[32m    839\u001B[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001B[32m    840\u001B[39m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m841\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mVariable\u001B[49m\u001B[43m.\u001B[49m\u001B[43m_execution_engine\u001B[49m\u001B[43m.\u001B[49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[32m    842\u001B[39m \u001B[43m        \u001B[49m\u001B[43mt_outputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\n\u001B[32m    843\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001B[39;00m\n\u001B[32m    844\u001B[39m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[32m    845\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m attach_logging_hooks:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 56
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
