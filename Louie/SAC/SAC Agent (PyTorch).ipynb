{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# SAC (Soft Actor-Critic) Implementation",
   "id": "d4f350f62dc83476"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-08T13:31:44.152371Z",
     "start_time": "2025-12-08T13:31:44.091875Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import time"
   ],
   "id": "cb2f681c4e04ed5d",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T13:31:45.249335Z",
     "start_time": "2025-12-08T13:31:45.141453Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class SACAgent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env_id,\n",
    "                 env_hardcore,\n",
    "                 critic_network1,\n",
    "                 critic_network2,\n",
    "                 actor_network,\n",
    "                 critic_learning_rate=3e-4,\n",
    "                 actor_learning_rate=3e-4,\n",
    "                 alpha_learning_rate=3e-4,\n",
    "                 log_alpha_start=0.0,\n",
    "                 discount_factor=0.99,\n",
    "                 minibatch_size=256,\n",
    "                 tau=0.005,\n",
    "                 random_exploration_steps=10_000,\n",
    "                 actor_exploration_steps=1000,\n",
    "                 critic_gradient_clip=1.0,\n",
    "                 actor_gradient_clip=1.0,\n",
    "                 alpha_gradient_clip=0.0,\n",
    "                 updates_per_step=1,\n",
    "                 max_buffer_length=1_000_000):\n",
    "        # CPU or GPU?\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Environment.\n",
    "        self.env_id = env_id\n",
    "        self.env_hardcore = env_hardcore\n",
    "        self.env = gym.make(id=env_id, hardcore=env_hardcore, render_mode=None)\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.shape[0]\n",
    "        self.action_min = torch.tensor(self.env.action_space.low, device=self.device)\n",
    "        self.action_max = torch.tensor(self.env.action_space.high, device=self.device)\n",
    "        self.max_reward = self.env.spec.reward_threshold\n",
    "\n",
    "        # Initialize critic network 1 and target critic network 1.\n",
    "        self.critic_network1 = copy.deepcopy(critic_network1)\n",
    "        self.target_critic_network1 = copy.deepcopy(critic_network1)\n",
    "\n",
    "        # Initialize critic network 2 and target critic network 2.\n",
    "        self.critic_network2 = copy.deepcopy(critic_network2)\n",
    "        self.target_critic_network2 = copy.deepcopy(critic_network2)\n",
    "\n",
    "        # Initialize actor network and target actor network.\n",
    "        self.actor_network = copy.deepcopy(actor_network)\n",
    "\n",
    "        # Move networks to correct device.\n",
    "        self.actor_network.to(self.device)\n",
    "        self.critic_network1.to(self.device)\n",
    "        self.critic_network2.to(self.device)\n",
    "        self.target_critic_network1.to(self.device)\n",
    "        self.target_critic_network2.to(self.device)\n",
    "\n",
    "        # Initialize entropy.\n",
    "        self.target_entropy = -self.action_size\n",
    "        self.log_alpha_start = log_alpha_start\n",
    "        self.log_alpha = nn.Parameter(torch.tensor(log_alpha_start, requires_grad=True, device=self.device))\n",
    "\n",
    "        # Initialize optimizers.\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.alpha_learning_rate = alpha_learning_rate\n",
    "        self.critic1_optimizer = optim.Adam(self.critic_network1.parameters(), lr=critic_learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic_network2.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=actor_learning_rate)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_learning_rate)\n",
    "\n",
    "        # Initialize hyperparameters.\n",
    "        self.tau = tau\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.random_exploration_steps = random_exploration_steps\n",
    "        self.actor_exploration_steps = actor_exploration_steps\n",
    "        self.critic_gradient_clip = critic_gradient_clip\n",
    "        self.actor_gradient_clip = actor_gradient_clip\n",
    "        self.alpha_gradient_clip = alpha_gradient_clip\n",
    "        self.updates_per_step = updates_per_step\n",
    "\n",
    "        # Initialize buffer.\n",
    "        self.buffer_width = 2 * self.state_size + self.action_size + 2\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.buffer_write_idx = 0\n",
    "        self.buffer_fullness = 0\n",
    "        self.buffer_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                         dtype=torch.float32,\n",
    "                                         device=self.device)\n",
    "        self.buffer_actions = torch.zeros((self.max_buffer_length, self.action_size),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_rewards = torch.zeros((self.max_buffer_length, 1),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_next_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                              dtype=torch.float32,\n",
    "                                              device=self.device)\n",
    "        self.buffer_terminals = torch.zeros((self.max_buffer_length, 1),\n",
    "                                            dtype=torch.float32,\n",
    "                                            device=self.device)\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action at the given state.\"\"\"\n",
    "        self.actor_network.eval()\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            action_tensor, _ = self.actor_network.sample(state_tensor)\n",
    "            return action_tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "    def save_transition(self, state: np.ndarray, action: np.ndarray, reward: float, new_state: np.ndarray, terminal: bool):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        self.buffer_states[self.buffer_write_idx] = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_actions[self.buffer_write_idx] = torch.tensor(action, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_rewards[self.buffer_write_idx] = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        self.buffer_next_states[self.buffer_write_idx] = torch.tensor(new_state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_terminals[self.buffer_write_idx] = torch.tensor([1.0 if terminal else 0.0],\n",
    "                                                                    dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.buffer_write_idx = (self.buffer_write_idx + 1) % self.max_buffer_length\n",
    "        self.buffer_fullness = min(self.buffer_fullness + 1, self.max_buffer_length)\n",
    "\n",
    "    def sample_minibatch(self):\n",
    "        \"\"\"Sample a minibatch from the replay buffer.\"\"\"\n",
    "        indices = torch.randint(0, self.buffer_fullness, (self.minibatch_size,), device=self.device)\n",
    "\n",
    "        mb_states = self.buffer_states[indices]\n",
    "        mb_actions = self.buffer_actions[indices]\n",
    "        mb_rewards = self.buffer_rewards[indices]\n",
    "        mb_next_states = self.buffer_next_states[indices]\n",
    "        mb_terminals = self.buffer_terminals[indices]\n",
    "\n",
    "        return mb_states, mb_actions, mb_rewards, mb_next_states, mb_terminals\n",
    "\n",
    "    def update_critic_networks(self, minibatch):\n",
    "        \"\"\"Update the critic networks.\"\"\"\n",
    "        mb_states, mb_actions, mb_rewards, mb_next_states, mb_terminals = minibatch\n",
    "        mb_state_actions = torch.cat([mb_states, mb_actions], dim=1)\n",
    "\n",
    "        self.actor_network.eval()\n",
    "        self.target_critic_network1.eval()\n",
    "        self.target_critic_network2.eval()\n",
    "        self.critic_network1.train()\n",
    "        self.critic_network2.train()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions, logp = self.actor_network.sample(mb_next_states)\n",
    "            next_state_actions = torch.cat((mb_next_states, next_actions), dim=1)\n",
    "            q1_next = self.target_critic_network1(next_state_actions)\n",
    "            q2_next = self.target_critic_network2(next_state_actions)\n",
    "            q_min_next = torch.min(q1_next, q2_next)\n",
    "            q_target = mb_rewards + self.discount_factor * (1 - mb_terminals) * (q_min_next - self.alpha * logp)\n",
    "\n",
    "        # Critic network 1 update.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network1(mb_state_actions)\n",
    "            critic1_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        if self.critic_gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.critic_network1.parameters(), self.critic_gradient_clip)\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        # Critic network 2 update.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network2(mb_state_actions)\n",
    "            critic2_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        if self.critic_gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.critic_network2.parameters(), self.critic_gradient_clip)\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "    def update_actor_network_and_alpha(self, minibatch):\n",
    "        \"\"\"Update the actor network.\"\"\"\n",
    "        mb_states, *_ = minibatch\n",
    "\n",
    "        self.actor_network.train()\n",
    "        self.critic_network1.eval()\n",
    "        self.critic_network2.eval()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            pred_actions, logp = self.actor_network.sample(mb_states)\n",
    "            pred_state_actions = torch.cat((mb_states, pred_actions), dim=1)\n",
    "            q1_pred = self.critic_network1(pred_state_actions)\n",
    "            q2_pred = self.critic_network2(pred_state_actions)\n",
    "            q_min_pred = torch.min(q1_pred, q2_pred)\n",
    "            actor_loss = -(q_min_pred - self.alpha * logp).mean()\n",
    "            alpha_loss = -(self.log_alpha * (logp + self.target_entropy).detach()).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        if self.actor_gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.actor_network.parameters(), self.actor_gradient_clip)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        if self.alpha_gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_([self.log_alpha], self.alpha_gradient_clip)\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "    def soft_update_target_critics(self):\n",
    "        \"\"\"Soft update the target networks weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for w_target, w_local in zip(self.target_critic_network1.parameters(), self.critic_network1.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "            for w_target, w_local in zip(self.target_critic_network2.parameters(), self.critic_network2.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "    def get_settings(self, n_episodes, stop_after):\n",
    "        return {\n",
    "            \"env_id\": self.env_id,\n",
    "            \"env_hardcore\": self.env_hardcore,\n",
    "            \"critic_learning_rate\": self.critic_learning_rate,\n",
    "            \"actor_learning_rate\": self.actor_learning_rate,\n",
    "            \"alpha_learning_rate\": self.alpha_learning_rate,\n",
    "            \"log_alpha_start\": self.log_alpha_start,\n",
    "            \"discount_factor\": self.discount_factor,\n",
    "            \"minibatch_size\": self.minibatch_size,\n",
    "            \"tau\": self.tau,\n",
    "            \"random_exploration_steps\": self.random_exploration_steps,\n",
    "            \"actor_exploration_steps\": self.actor_exploration_steps,\n",
    "            \"critic_gradient_clip\": self.critic_gradient_clip,\n",
    "            \"actor_gradient_clip\": self.actor_gradient_clip,\n",
    "            \"alpha_gradient_clip\": self.alpha_gradient_clip,\n",
    "            \"updates_per_step\": self.updates_per_step,\n",
    "            \"max_buffer_length\": self.max_buffer_length,\n",
    "            \"n_episodes\": n_episodes,\n",
    "            \"stop_after\": stop_after\n",
    "        }\n",
    "\n",
    "    def save_outputs(self, episode_rewards, episode_step_counts, episode_run_times, episode_alphas, n_episodes, stop_after):\n",
    "        \"\"\"Save each network's weights and episode rewards.\"\"\"\n",
    "        torch.save(self.critic_network1.state_dict(), \"critic_network1.pth\")\n",
    "        torch.save(self.critic_network2.state_dict(), \"critic_network2.pth\")\n",
    "        torch.save(self.actor_network.state_dict(), \"actor_network.pth\")\n",
    "        with open(\"episode_rewards.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_rewards, fp)\n",
    "        with open(\"episode_step_counts.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_step_counts, fp)\n",
    "        with open(\"episode_run_times.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_run_times, fp)\n",
    "        with open(\"episode_alphas.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_alphas, fp)\n",
    "        with open(\"settings.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(self.get_settings(n_episodes, stop_after), fp)\n",
    "\n",
    "    def show_test_episode(self):\n",
    "        \"\"\"Do a visual test run.\"\"\"\n",
    "        print(\"\\n========TEST RUN========\")\n",
    "        test_env = gym.make(id=self.env_id, hardcore=self.env_hardcore, render_mode=\"human\")\n",
    "        s, _ = test_env.reset()\n",
    "        test_episode_reward = 0\n",
    "        test_episode_step_count = 0\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            a = self.select_action(s)\n",
    "            s_, r, terminal, truncated, _ = test_env.step(a)\n",
    "            test_episode_reward += r\n",
    "            test_episode_step_count += 1\n",
    "\n",
    "            if terminal or truncated:\n",
    "                break\n",
    "\n",
    "            s = s_\n",
    "\n",
    "        test_episode_end_time = time.time()\n",
    "        test_episode_run_time = test_episode_end_time - test_start_time\n",
    "        test_env.close()\n",
    "\n",
    "        print(f\"Reward: {test_episode_reward:.2f} - Step Count: {test_episode_step_count} - Run Time: {test_episode_run_time:.2f}s\\n\")\n",
    "\n",
    "    def random_exploration(self):\n",
    "        print(\"Performing Random Exploration...\")\n",
    "        step_count = 0\n",
    "        temp_env = gym.make(id=self.env_id, hardcore=self.env_hardcore, render_mode=None)\n",
    "        state, _ = temp_env.reset()\n",
    "\n",
    "        while step_count < self.random_exploration_steps:\n",
    "\n",
    "            action_tensor = self.action_min + (self.action_max - self.action_min) * torch.rand(self.action_size, device=self.device)\n",
    "            action = action_tensor.cpu().numpy()\n",
    "            new_state, reward, terminal, truncated, _ = temp_env.step(action)\n",
    "            self.save_transition(state, action, reward, new_state, terminal)\n",
    "\n",
    "            step_count += 1\n",
    "            state = new_state\n",
    "\n",
    "            if terminal or truncated:\n",
    "                state, _ = temp_env.reset()\n",
    "\n",
    "        temp_env.close()\n",
    "        print(\"Random Exploration Complete.\")\n",
    "\n",
    "    def actor_exploration(self):\n",
    "        print(\"Performing Actor Exploration...\")\n",
    "        step_count = 0\n",
    "        temp_env = gym.make(id=self.env_id, hardcore=self.env_hardcore, render_mode=None)\n",
    "        state, _ = temp_env.reset()\n",
    "\n",
    "        while step_count < self.actor_exploration_steps:\n",
    "\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, terminal, truncated, _ = temp_env.step(action)\n",
    "            self.save_transition(state, action, reward, new_state, terminal)\n",
    "\n",
    "            step_count += 1\n",
    "            state = new_state\n",
    "\n",
    "            if terminal or truncated:\n",
    "                state, _ = temp_env.reset()\n",
    "\n",
    "        temp_env.close()\n",
    "        print(\"Actor Exploration Complete.\")\n",
    "\n",
    "    def learn(self, n_episodes=2000, display_every=50, stop_after=None):\n",
    "\n",
    "        self.random_exploration()\n",
    "        self.actor_exploration()\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_step_counts = []\n",
    "        episode_run_times = []\n",
    "        episode_alphas = []\n",
    "\n",
    "        total_step_count = 0\n",
    "\n",
    "        for n in range(n_episodes):\n",
    "            # Print episode number.\n",
    "            print(f\"Running Episode {n + 1}...\")\n",
    "\n",
    "            start_time = time.time()\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_step_count = 0\n",
    "\n",
    "            while True:\n",
    "                # Select action and take step.\n",
    "                action = self.select_action(state)\n",
    "                new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Store transition.\n",
    "                self.save_transition(state, action, reward, new_state, terminal)\n",
    "\n",
    "                # Update step count.\n",
    "                episode_step_count += 1\n",
    "                total_step_count += 1\n",
    "\n",
    "                # Update episode reward.\n",
    "                episode_reward += reward\n",
    "\n",
    "                if self.buffer_fullness >= self.minibatch_size:\n",
    "\n",
    "                    for _ in range(self.updates_per_step):\n",
    "\n",
    "                        minibatch = self.sample_minibatch()\n",
    "                        self.update_critic_networks(minibatch)\n",
    "                        self.update_actor_network_and_alpha(minibatch)\n",
    "                        self.soft_update_target_critics()\n",
    "\n",
    "                if terminal or truncated:\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            end_time = time.time()\n",
    "            episode_run_time = end_time - start_time\n",
    "\n",
    "            # Print and save episode reward.\n",
    "            print(f\"Reward: {episode_reward:.2f} - Step Count: {episode_step_count} - Run Time: {episode_run_time:.2f}s - Alpha: {self.alpha:.5f}\")\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_step_counts.append(episode_step_count)\n",
    "            episode_run_times.append(episode_run_time)\n",
    "            episode_alphas.append(self.alpha.item())\n",
    "\n",
    "            # Early stopping.\n",
    "            if stop_after is not None and all(ep_rew >= self.max_reward for ep_rew in episode_rewards[-stop_after:]):\n",
    "                break\n",
    "\n",
    "            if n % display_every == 0:\n",
    "                self.show_test_episode()\n",
    "\n",
    "        self.save_outputs(episode_rewards, episode_step_counts, episode_run_times, episode_alphas, n_episodes, stop_after)"
   ],
   "id": "50fb56baa120e2b7",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T13:31:54.964923Z",
     "start_time": "2025-12-08T13:31:54.952316Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "63a7ea468b278b9f",
   "outputs": [],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T13:31:55.302584Z",
     "start_time": "2025-12-08T13:31:55.292478Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(24, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mean_linear = nn.Linear(256, 4)\n",
    "        self.log_std_linear = nn.Linear(256, 4)\n",
    "\n",
    "        self.log_std_min = -20\n",
    "        self.log_std_max = 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, x):\n",
    "        mean, log_std = self.forward(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        a_raw = normal.rsample()\n",
    "        action = torch.tanh(a_raw)\n",
    "\n",
    "        log_prob = normal.log_prob(a_raw) - torch.log(1 - action ** 2 + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob"
   ],
   "id": "69493f7fd0446c31",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training - Stage 1 - Complete Normal Mode",
   "id": "886e0bc71aaf12f4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train robot to complete normal.\n",
    "\n",
    "actor = Actor()\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=actor)\n",
    "\n",
    "agent.learn(n_episodes=2000, stop_after=10)"
   ],
   "id": "2e2c406983365052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T10:50:31.924938Z",
     "start_time": "2025-12-08T10:50:31.917654Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print hyperparameters used.\n",
    "\n",
    "with open(\"stage 1/settings.pkl\", \"rb\") as f:\n",
    "    stage1_settings = pickle.load(f)\n",
    "\n",
    "print(stage1_settings)"
   ],
   "id": "46bcefc1de732d76",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_id': 'BipedalWalker-v3', 'env_hardcore': False, 'critic_learning_rate': 0.0003, 'actor_learning_rate': 0.0003, 'alpha_learning_rate': 0.0003, 'log_alpha_start': 0.0, 'discount_factor': 0.99, 'minibatch_size': 256, 'tau': 0.005, 'random_exploration_steps': 10000, 'actor_exploration_steps': 1000, 'critic_gradient_clip': 1.0, 'actor_gradient_clip': 1.0, 'alpha_gradient_clip': 0.0, 'updates_per_step': 1, 'max_buffer_length': 250000, 'n_episodes': 2000, 'stop_after': 10}\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T10:51:50.681940Z",
     "start_time": "2025-12-08T10:51:28.663856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Watch an episode of trained robot.\n",
    "\n",
    "trained_actor = Actor()\n",
    "trained_actor.load_state_dict(torch.load(\"stage 1/actor_network.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=trained_actor)\n",
    "\n",
    "agent.show_test_episode()"
   ],
   "id": "4ac4a9f010dc4e31",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========TEST RUN========\n",
      "Reward: 304.69 - Step Count: 958 - Run Time: 21.22s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training - Stage 2 - Faster in Normal Mode",
   "id": "4aa2adadd33ef22f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train robot to go faster in normal.\n",
    "\n",
    "stage1_actor = Actor()\n",
    "stage1_actor.load_state_dict(torch.load(\"stage 1/actor_network.pth\"))\n",
    "\n",
    "stage1_critic1 = Critic()\n",
    "stage1_critic1.load_state_dict(torch.load(\"stage 1/critic_network1.pth\"))\n",
    "\n",
    "stage1_critic2 = Critic()\n",
    "stage1_critic2.load_state_dict(torch.load(\"stage 1/critic_network2.pth\"))\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=stage1_critic1,\n",
    "                 critic_network2=stage1_critic2,\n",
    "                 actor_network=stage1_actor,\n",
    "                 log_alpha_start=-4.605)\n",
    "\n",
    "agent.learn(n_episodes=300, stop_after=None)"
   ],
   "id": "89fdd7a46eca8466"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T10:56:26.998653Z",
     "start_time": "2025-12-08T10:56:26.992200Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print hyperparameters used.\n",
    "\n",
    "with open(\"stage 2/settings.pkl\", \"rb\") as f:\n",
    "    stage2_settings = pickle.load(f)\n",
    "\n",
    "print(stage2_settings)"
   ],
   "id": "764b3d18d8f0f6f4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_id': 'BipedalWalker-v3', 'env_hardcore': False, 'critic_learning_rate': 0.0003, 'actor_learning_rate': 0.0003, 'alpha_learning_rate': 0.0003, 'log_alpha_start': -4.605, 'discount_factor': 0.99, 'minibatch_size': 256, 'tau': 0.005, 'random_exploration_steps': 1000, 'actor_exploration_steps': 10000, 'critic_gradient_clip': 1.0, 'actor_gradient_clip': 1.0, 'alpha_gradient_clip': 0.0, 'updates_per_step': 1, 'max_buffer_length': 250000, 'n_episodes': 300, 'stop_after': None}\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T10:56:52.028572Z",
     "start_time": "2025-12-08T10:56:36.100165Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Watch an episode of trained robot.\n",
    "\n",
    "trained_actor = Actor()\n",
    "trained_actor.load_state_dict(torch.load(\"stage 2/actor_network.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=trained_actor)\n",
    "\n",
    "agent.show_test_episode()"
   ],
   "id": "14ce5c8276d1cb36",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========TEST RUN========\n",
      "Reward: 322.44 - Step Count: 736 - Run Time: 15.10s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training - Stage 3 - Complete Hardcore Mode",
   "id": "59918d497c8a5c33"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train robot to complete hardcore.\n",
    "\n",
    "stage2_actor = Actor()\n",
    "stage2_actor.load_state_dict(torch.load(\"stage 2/actor_network.pth\"))\n",
    "\n",
    "stage2_critic1 = Critic()\n",
    "stage2_critic1.load_state_dict(torch.load(\"stage 2/critic_network1.pth\"))\n",
    "\n",
    "stage2_critic2 = Critic()\n",
    "stage2_critic2.load_state_dict(torch.load(\"stage 2/critic_network2.pth\"))\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=True,\n",
    "                 critic_network1=stage2_critic1,\n",
    "                 critic_network2=stage2_critic2,\n",
    "                 actor_network=stage2_actor,\n",
    "                 log_alpha_start=-4.605)\n",
    "\n",
    "agent.learn(n_episodes=2000, stop_after=3)"
   ],
   "id": "2b3ffeafae9507ec"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T10:58:54.735082Z",
     "start_time": "2025-12-08T10:58:54.728627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print hyperparameters used.\n",
    "\n",
    "with open(\"stage 3/settings.pkl\", \"rb\") as f:\n",
    "    stage3_settings = pickle.load(f)\n",
    "\n",
    "print(stage3_settings)"
   ],
   "id": "6f8de7c2bd836501",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_id': 'BipedalWalker-v3', 'env_hardcore': True, 'critic_learning_rate': 0.0003, 'actor_learning_rate': 0.0003, 'alpha_learning_rate': 0.0003, 'log_alpha_start': -4.605, 'discount_factor': 0.99, 'minibatch_size': 256, 'tau': 0.005, 'random_exploration_steps': 1000, 'actor_exploration_steps': 10000, 'critic_gradient_clip': 1.0, 'actor_gradient_clip': 1.0, 'alpha_gradient_clip': 0.0, 'updates_per_step': 1, 'max_buffer_length': 250000, 'n_episodes': 2000, 'stop_after': 3}\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T11:00:06.591761Z",
     "start_time": "2025-12-08T10:59:32.788417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "trained_actor = Actor()\n",
    "trained_actor.load_state_dict(torch.load(\"stage 3/actor_network.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=True,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=trained_actor)\n",
    "\n",
    "agent.show_test_episode()"
   ],
   "id": "d2a36d187b221ca",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========TEST RUN========\n",
      "Reward: 180.33 - Step Count: 1600 - Run Time: 33.05s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
