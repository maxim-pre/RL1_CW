{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import time"
   ],
   "id": "a519b53e8f66d65d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class DDPGAgent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env_id,\n",
    "                 env_hardcore,\n",
    "                 critic_network,\n",
    "                 actor_network,\n",
    "                 critic_learning_rate=1e-3,\n",
    "                 actor_learning_rate=1e-4,\n",
    "                 discount_factor=0.99,\n",
    "                 minibatch_size=256,\n",
    "                 tau=0.005,\n",
    "                 exploratory_noise=0.1,\n",
    "                 exploratory_noise_clip=0.3,\n",
    "                 warm_up=10_000,\n",
    "                 gradient_clip=1.0,\n",
    "                 updates_per_step=1,\n",
    "                 max_buffer_length=250_000):\n",
    "        # CPU or GPU?\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Environment.\n",
    "        self.env_id = env_id\n",
    "        self.env_hardcore = env_hardcore\n",
    "        self.env = gym.make(id=env_id, hardcore=env_hardcore, render_mode=None)\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.shape[0]\n",
    "        self.action_min = torch.tensor(self.env.action_space.low, device=self.device)\n",
    "        self.action_max = torch.tensor(self.env.action_space.high, device=self.device)\n",
    "        self.max_reward = self.env.spec.reward_threshold\n",
    "\n",
    "        # Initialize critic network and target critic network.\n",
    "        self.critic_network = copy.deepcopy(critic_network)\n",
    "        self.target_critic_network = copy.deepcopy(critic_network)\n",
    "\n",
    "        # Initialize actor network and target actor network.\n",
    "        self.actor_network = copy.deepcopy(actor_network)\n",
    "        self.target_actor_network = copy.deepcopy(actor_network)\n",
    "\n",
    "        # Move networks to correct device.\n",
    "        self.actor_network.to(self.device)\n",
    "        self.critic_network.to(self.device)\n",
    "        self.target_actor_network.to(self.device)\n",
    "        self.target_critic_network.to(self.device)\n",
    "\n",
    "        # Initialize optimizers.\n",
    "        self.critic_optimizer = optim.Adam(self.critic_network.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=actor_learning_rate)\n",
    "\n",
    "        # Initialize hyperparameters.\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.exploratory_noise = exploratory_noise\n",
    "        self.warm_up = warm_up\n",
    "        self.exploratory_noise_clip = exploratory_noise_clip\n",
    "        self.gradient_clip = gradient_clip\n",
    "        self.updates_per_step = updates_per_step\n",
    "\n",
    "        # Initialize buffer.\n",
    "        self.buffer_width = 2 * self.state_size + self.action_size + 2\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.buffer_write_idx = 0\n",
    "        self.buffer_fullness = 0\n",
    "        self.buffer_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                         dtype=torch.float32,\n",
    "                                         device=self.device)\n",
    "        self.buffer_actions = torch.zeros((self.max_buffer_length, self.action_size),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_rewards = torch.zeros((self.max_buffer_length, 1),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_next_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                              dtype=torch.float32,\n",
    "                                              device=self.device)\n",
    "        self.buffer_terminals = torch.zeros((self.max_buffer_length, 1),\n",
    "                                            dtype=torch.float32,\n",
    "                                            device=self.device)\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action at the given state.\"\"\"\n",
    "        self.actor_network.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass.\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            action_tensor = self.actor_network(state_tensor).squeeze(0)\n",
    "            # Add exploratory noise.\n",
    "            noise = torch.randn(self.action_size, device=self.device) * self.exploratory_noise\n",
    "            noise_clipped = torch.clamp(noise, min=-self.exploratory_noise_clip, max=self.exploratory_noise_clip)\n",
    "            action_tensor = action_tensor + noise_clipped\n",
    "            # Clip action.\n",
    "            action_tensor = torch.clamp(action_tensor, min=self.action_min, max=self.action_max)\n",
    "            return action_tensor.cpu().numpy()\n",
    "\n",
    "    def save_transition(self, state: np.ndarray, action: np.ndarray, reward: float, new_state: np.ndarray, terminal: bool):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        # Save transition.\n",
    "        self.buffer_states[self.buffer_write_idx] = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_actions[self.buffer_write_idx] = torch.tensor(action, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_rewards[self.buffer_write_idx] = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        self.buffer_next_states[self.buffer_write_idx] = torch.tensor(new_state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_terminals[self.buffer_write_idx] = torch.tensor([1.0 if terminal else 0.0],\n",
    "                                                                    dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.buffer_write_idx = (self.buffer_write_idx + 1) % self.max_buffer_length\n",
    "        self.buffer_fullness = min(self.buffer_fullness + 1, self.max_buffer_length)\n",
    "\n",
    "    def sample_minibatch(self):\n",
    "        \"\"\"Sample a minibatch from the replay buffer.\"\"\"\n",
    "        indices = torch.randint(0, self.buffer_fullness, (self.minibatch_size,), device=self.device)\n",
    "\n",
    "        mb_states = self.buffer_states[indices]\n",
    "        mb_actions = self.buffer_actions[indices]\n",
    "        mb_rewards = self.buffer_rewards[indices]\n",
    "        mb_next_states = self.buffer_next_states[indices]\n",
    "        mb_terminals = self.buffer_terminals[indices]\n",
    "\n",
    "        return mb_states, mb_actions, mb_rewards, mb_next_states, mb_terminals\n",
    "\n",
    "    def update_critic_network(self, minibatch: torch.Tensor):\n",
    "        \"\"\"Update the critic network.\"\"\"\n",
    "        mb_states, mb_actions, mb_rewards, mb_next_states, mb_terminals = minibatch\n",
    "        mb_state_actions = torch.cat([mb_states, mb_actions], dim=1)\n",
    "\n",
    "        self.target_actor_network.eval()\n",
    "        self.target_critic_network.eval()\n",
    "        self.critic_network.train()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor_network(mb_next_states)\n",
    "            next_state_actions = torch.cat((mb_next_states, next_actions), dim=1)\n",
    "            q_next = self.target_critic_network(next_state_actions)\n",
    "            q_target = mb_rewards + self.discount_factor * (1.0 - mb_terminals) * q_next\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network(mb_state_actions)\n",
    "            critic_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        if self.gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.critic_network.parameters(), self.gradient_clip)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "    def update_actor_network(self, minibatch: torch.Tensor):\n",
    "        \"\"\"Update the actor network.\"\"\"\n",
    "        mb_states, *_ = minibatch\n",
    "\n",
    "        self.actor_network.train()\n",
    "        self.critic_network.eval()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            raw_actions = self.actor_network(mb_states)\n",
    "            raw_state_actions = torch.cat((mb_states, raw_actions), dim=1)\n",
    "            actor_loss = -self.critic_network(raw_state_actions).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        if self.gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.actor_network.parameters(), self.gradient_clip)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def soft_update_target_weights(self):\n",
    "        \"\"\"Soft update the target networks weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for w_target, w_local in zip(self.target_actor_network.parameters(), self.actor_network.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "            for w_target, w_local in zip(self.target_critic_network.parameters(), self.critic_network.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "    def save_outputs(self, episode_rewards, episode_step_counts, episode_run_times):\n",
    "        \"\"\"Save each network's weights and episode rewards.\"\"\"\n",
    "        folder = \"DDPG Outputs (hardcore)\" if self.env_hardcore else \"DDPG Outputs (normal)\"\n",
    "        torch.save(self.critic_network.state_dict(), folder + \"/critic_network.pth\")\n",
    "        torch.save(self.actor_network.state_dict(), folder + \"/actor_network.pth\")\n",
    "        with open(folder + \"/episode_rewards.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_rewards, fp)\n",
    "        with open(folder + \"/episode_step_counts.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_step_counts, fp)\n",
    "        with open(folder + \"/episode_run_times.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_run_times, fp)\n",
    "\n",
    "    def show_test_episode(self):\n",
    "        \"\"\"Do a visual test run.\"\"\"\n",
    "        print(\"\\n========TEST RUN========\")\n",
    "        test_env = gym.make(id=self.env_id, hardcore=self.env_hardcore, render_mode=\"human\")\n",
    "        s, _ = test_env.reset()\n",
    "        test_episode_reward = 0\n",
    "        test_episode_step_count = 0\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            a = self.select_action(s)\n",
    "            s_, r, terminated, truncated, _ = test_env.step(a)\n",
    "            test_episode_reward += r\n",
    "            test_episode_step_count += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            s = s_\n",
    "\n",
    "        test_episode_end_time = time.time()\n",
    "        test_episode_run_time = test_episode_end_time - test_start_time\n",
    "        test_env.close()\n",
    "\n",
    "        print(f\"Reward: {test_episode_reward:.2f} - Step Count: {test_episode_step_count} - Run Time: {test_episode_run_time:.2f}s\\n\")\n",
    "\n",
    "    def learn(self, n_episodes=2000, display_every=50, stop_after=None):\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_step_counts = []\n",
    "        episode_run_times = []\n",
    "\n",
    "        for n in range(n_episodes):\n",
    "            # Print episode number.\n",
    "            print(f\"Running Episode {n + 1}...\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Reset environment.\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_step_count = 0\n",
    "\n",
    "            while True:\n",
    "                # Select action and take step.\n",
    "                action = self.select_action(state)\n",
    "                new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Store transition.\n",
    "                self.save_transition(state, action, reward, new_state, terminal)\n",
    "\n",
    "                # Update episode reward\n",
    "                episode_step_count += 1\n",
    "                episode_reward += reward\n",
    "\n",
    "                if self.buffer_fullness >= self.minibatch_size and self.buffer_fullness >= self.warm_up:\n",
    "\n",
    "                    for _ in range(self.updates_per_step):\n",
    "\n",
    "                        minibatch = self.sample_minibatch()\n",
    "                        self.update_critic_network(minibatch)\n",
    "                        self.update_actor_network(minibatch)\n",
    "                        self.soft_update_target_weights()\n",
    "\n",
    "                if terminal or truncated:\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            end_time = time.time()\n",
    "            episode_run_time = end_time - start_time\n",
    "\n",
    "            # Print and save episode reward.\n",
    "            print(f\"Reward: {episode_reward:.2f} - Step Count: {episode_step_count} - Run Time: {episode_run_time:.2f}s\")\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_step_counts.append(episode_step_count)\n",
    "            episode_run_times.append(episode_run_time)\n",
    "\n",
    "            # Early stopping.\n",
    "            if stop_after is not None and all(ep_rew >= self.max_reward for ep_rew in episode_rewards[-stop_after:]):\n",
    "                break\n",
    "\n",
    "            if n % display_every == 0:\n",
    "                self.show_test_episode()\n",
    "\n",
    "        self.save_outputs(episode_rewards, episode_step_counts, episode_run_times)"
   ],
   "id": "1572d57c181f46d6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "b3d2857fc6e434f7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(24, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "eaaa1bd8ebf11e78",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the robot.\n",
    "\n",
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "agent = DDPGAgent(env_id=\"BipedalWalker-v3\",\n",
    "                  env_hardcore=False,\n",
    "                  critic_network=critic,\n",
    "                  actor_network=actor)\n",
    "\n",
    "agent.learn(stop_after=1)"
   ],
   "id": "ada21abcdd639c16",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
