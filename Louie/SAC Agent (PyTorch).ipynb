{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import time"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class SACAgent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env_id,\n",
    "                 env_hardcore,\n",
    "                 critic_network1,\n",
    "                 critic_network2,\n",
    "                 actor_network,\n",
    "                 critic_learning_rate=3e-4,\n",
    "                 actor_learning_rate=3e-4,\n",
    "                 alpha_learning_rate=3e-4,\n",
    "                 log_std_start=0.0,\n",
    "                 discount_factor=0.99,\n",
    "                 minibatch_size=256,\n",
    "                 tau=0.005,\n",
    "                 warm_up=10_000,\n",
    "                 updates_per_step=2,\n",
    "                 actor_update_freq=2,\n",
    "                 critic_update_freq=4,\n",
    "                 gradient_clip=1.0,\n",
    "                 max_buffer_length=250_000):\n",
    "        # CPU or GPU?\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Environment.\n",
    "        self.env_id = env_id\n",
    "        self.env_hardcore = env_hardcore\n",
    "        self.env = gym.make(id=env_id, hardcore=env_hardcore, render_mode=None)\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.shape[0]\n",
    "        self.action_min = torch.tensor(self.env.action_space.low, device=self.device)\n",
    "        self.action_max = torch.tensor(self.env.action_space.high, device=self.device)\n",
    "        self.max_reward = self.env.spec.reward_threshold\n",
    "\n",
    "        # Initialize critic network 1 and target critic network 1.\n",
    "        self.critic_network1 = copy.deepcopy(critic_network1)\n",
    "        self.target_critic_network1 = copy.deepcopy(critic_network1)\n",
    "\n",
    "        # Initialize critic network 2 and target critic network 2.\n",
    "        self.critic_network2 = copy.deepcopy(critic_network2)\n",
    "        self.target_critic_network2 = copy.deepcopy(critic_network2)\n",
    "\n",
    "        # Initialize actor network and target actor network.\n",
    "        self.actor_network = copy.deepcopy(actor_network)\n",
    "\n",
    "        # Move networks to correct device.\n",
    "        self.actor_network.to(self.device)\n",
    "        self.critic_network1.to(self.device)\n",
    "        self.critic_network2.to(self.device)\n",
    "        self.target_critic_network1.to(self.device)\n",
    "        self.target_critic_network2.to(self.device)\n",
    "\n",
    "        # Initialize entropy.\n",
    "        self.target_entropy = -self.action_size\n",
    "        self.log_alpha = torch.tensor(log_std_start, requires_grad=True, device=self.device)\n",
    "\n",
    "        # Initialize optimizers.\n",
    "        self.critic1_optimizer = optim.Adam(self.critic_network1.parameters(), lr=critic_learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic_network2.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=actor_learning_rate)\n",
    "        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=alpha_learning_rate)\n",
    "\n",
    "        # Initialize hyperparameters.\n",
    "        self.tau = tau\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.warm_up = warm_up\n",
    "        self.updates_per_step = updates_per_step\n",
    "        self.actor_update_freq = actor_update_freq\n",
    "        self.critic_update_freq = critic_update_freq\n",
    "        self.gradient_clip = gradient_clip\n",
    "\n",
    "        # Initialize buffer.\n",
    "        self.buffer_width = 2 * self.state_size + self.action_size + 2\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.buffer_write_idx = 0\n",
    "        self.buffer_fullness = 0\n",
    "        self.buffer_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                         dtype=torch.float32,\n",
    "                                         device=self.device)\n",
    "        self.buffer_actions = torch.zeros((self.max_buffer_length, self.action_size),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_rewards = torch.zeros((self.max_buffer_length, 1),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_next_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                              dtype=torch.float32,\n",
    "                                              device=self.device)\n",
    "        self.buffer_terminals = torch.zeros((self.max_buffer_length, 1),\n",
    "                                            dtype=torch.float32,\n",
    "                                            device=self.device)\n",
    "\n",
    "    @property\n",
    "    def alpha(self):\n",
    "        return self.log_alpha.exp()\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action at the given state.\"\"\"\n",
    "        self.actor_network.eval()\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            action_tensor, _ = self.actor_network.sample(state_tensor)\n",
    "            return action_tensor.squeeze(0).cpu().numpy()\n",
    "\n",
    "    def save_transition(self, state: np.ndarray, action: np.ndarray, reward: float, new_state: np.ndarray, terminal: bool):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        self.buffer_states[self.buffer_write_idx] = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_actions[self.buffer_write_idx] = torch.tensor(action, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_rewards[self.buffer_write_idx] = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        self.buffer_next_states[self.buffer_write_idx] = torch.tensor(new_state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_terminals[self.buffer_write_idx] = torch.tensor([1.0 if terminal else 0.0],\n",
    "                                                                    dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.buffer_write_idx = (self.buffer_write_idx + 1) % self.max_buffer_length\n",
    "        self.buffer_fullness = min(self.buffer_fullness + 1, self.max_buffer_length)\n",
    "\n",
    "    def sample_minibatch(self):\n",
    "        \"\"\"Sample a minibatch from the replay buffer.\"\"\"\n",
    "        indices = torch.randint(0, self.buffer_fullness, (self.minibatch_size,), device=self.device)\n",
    "\n",
    "        mb_states = self.buffer_states[indices]\n",
    "        mb_actions = self.buffer_actions[indices]\n",
    "        mb_rewards = self.buffer_rewards[indices]\n",
    "        mb_next_states = self.buffer_next_states[indices]\n",
    "        mb_terminals = self.buffer_terminals[indices]\n",
    "\n",
    "        return mb_states, mb_actions, mb_rewards, mb_next_states, mb_terminals\n",
    "\n",
    "    def update_critic_networks(self, minibatch):\n",
    "        \"\"\"Update the critic networks.\"\"\"\n",
    "        mb_states, mb_actions, mb_rewards, mb_next_states, mb_terminals = minibatch\n",
    "        mb_state_actions = torch.cat([mb_states, mb_actions], dim=1)\n",
    "\n",
    "        self.actor_network.eval()\n",
    "        self.target_critic_network1.eval()\n",
    "        self.target_critic_network2.eval()\n",
    "        self.critic_network1.train()\n",
    "        self.critic_network2.train()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions, logp = self.actor_network.sample(mb_next_states)\n",
    "            next_state_actions = torch.cat((mb_next_states, next_actions), dim=1)\n",
    "            q1_next = self.target_critic_network1(next_state_actions)\n",
    "            q2_next = self.target_critic_network2(next_state_actions)\n",
    "            q_min_next = torch.min(q1_next, q2_next)\n",
    "            q_target = mb_rewards + self.discount_factor * (1 - mb_terminals) * (q_min_next - self.alpha * logp)\n",
    "\n",
    "        # Critic network 1 update.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network1(mb_state_actions)\n",
    "            critic1_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        if self.gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.critic_network1.parameters(), self.gradient_clip)\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        # Critic network 2 update.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network2(mb_state_actions)\n",
    "            critic2_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        nn.utils.clip_grad_norm_(self.critic_network2.parameters(), self.gradient_clip)\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "    def update_actor_network(self, minibatch):\n",
    "        \"\"\"Update the actor network.\"\"\"\n",
    "        mb_states, *_ = minibatch\n",
    "\n",
    "        self.actor_network.train()\n",
    "        self.critic_network1.eval()\n",
    "        self.critic_network2.eval()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            pred_actions, logp = self.actor_network.sample(mb_states)\n",
    "            pred_state_actions = torch.cat((mb_states, pred_actions), dim=1)\n",
    "            q1_pred = self.critic_network1(pred_state_actions)\n",
    "            q2_pred = self.critic_network2(pred_state_actions)\n",
    "            q_min_pred = torch.min(q1_pred, q2_pred)\n",
    "            actor_loss = -(q_min_pred - self.alpha * logp).mean()\n",
    "            alpha_loss = -(self.log_alpha * (logp + self.target_entropy).detach()).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        if self.gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.actor_network.parameters(), self.gradient_clip)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.alpha_optimizer.zero_grad()\n",
    "        alpha_loss.backward()\n",
    "        self.alpha_optimizer.step()\n",
    "\n",
    "    def soft_update_target_critics(self):\n",
    "        \"\"\"Soft update the target networks weights.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            for w_target, w_local in zip(self.target_critic_network1.parameters(), self.critic_network1.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "            for w_target, w_local in zip(self.target_critic_network2.parameters(), self.critic_network2.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "    def save_outputs(self, episode_rewards, episode_step_counts, episode_run_times, episode_alphas):\n",
    "        \"\"\"Save each network's weights and episode rewards.\"\"\"\n",
    "        folder = \"SAC Outputs (hardcore)\" if self.env_hardcore else \"SAC Outputs (normal)\"\n",
    "        torch.save(self.critic_network1.state_dict(), folder + \"/critic_network1.pth\")\n",
    "        torch.save(self.critic_network2.state_dict(), folder + \"/critic_network2.pth\")\n",
    "        torch.save(self.actor_network.state_dict(), folder + \"/actor_network.pth\")\n",
    "        with open(folder + \"/episode_rewards.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_rewards, fp)\n",
    "        with open(folder + \"/episode_step_counts.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_step_counts, fp)\n",
    "        with open(folder + \"/episode_run_times.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_run_times, fp)\n",
    "        with open(folder + \"/episode_alphas.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_alphas, fp)\n",
    "\n",
    "    def show_test_episode(self):\n",
    "        \"\"\"Do a visual test run.\"\"\"\n",
    "        print(\"\\n========TEST RUN========\")\n",
    "        test_env = gym.make(id=self.env_id, hardcore=self.env_hardcore, render_mode=\"human\")\n",
    "        s, _ = test_env.reset()\n",
    "        test_episode_reward = 0\n",
    "        test_episode_step_count = 0\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            a = self.select_action(s)\n",
    "            s_, r, terminated, truncated, _ = test_env.step(a)\n",
    "            test_episode_reward += r\n",
    "            test_episode_step_count += 1\n",
    "\n",
    "            if terminated or truncated:\n",
    "                break\n",
    "\n",
    "            s = s_\n",
    "\n",
    "        test_episode_end_time = time.time()\n",
    "        test_episode_run_time = test_episode_end_time - test_start_time\n",
    "        test_env.close()\n",
    "\n",
    "        print(f\"Reward: {test_episode_reward:.2f} - Step Count: {test_episode_step_count} - Run Time: {test_episode_run_time:.2f}s - Alpha: {self.alpha:.5f}\\n\")\n",
    "\n",
    "    def learn(self, n_episodes=2000, display_every=50, stop_after=None):\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_step_counts = []\n",
    "        episode_run_times = []\n",
    "        episode_alphas = []\n",
    "\n",
    "        critic_update_count = 0\n",
    "        total_step_count = 0\n",
    "\n",
    "        for n in range(n_episodes):\n",
    "            # Print episode number.\n",
    "            print(f\"Running Episode {n + 1}...\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Reset environment.\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_step_count = 0\n",
    "\n",
    "            while True:\n",
    "                # Select action and take step.\n",
    "                action = self.select_action(state)\n",
    "                new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "\n",
    "                # Store transition.\n",
    "                self.save_transition(state, action, reward, new_state, terminal)\n",
    "\n",
    "                # Update step count.\n",
    "                total_step_count += 1\n",
    "                episode_step_count += 1\n",
    "\n",
    "                # Update episode reward.\n",
    "                episode_reward += reward\n",
    "\n",
    "                if self.buffer_fullness >= self.minibatch_size and self.buffer_fullness >= self.warm_up:\n",
    "\n",
    "                    if total_step_count % self.critic_update_freq == 0:\n",
    "\n",
    "                        for _ in range(self.updates_per_step):\n",
    "\n",
    "                            minibatch = self.sample_minibatch()\n",
    "                            self.update_critic_networks(minibatch)\n",
    "                            self.soft_update_target_critics()\n",
    "                            critic_update_count += 1\n",
    "\n",
    "                            if critic_update_count % self.actor_update_freq == 0:\n",
    "                                self.update_actor_network(minibatch)\n",
    "\n",
    "                if terminal or truncated:\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            end_time = time.time()\n",
    "            episode_run_time = end_time - start_time\n",
    "\n",
    "            # Print and save episode reward.\n",
    "            print(f\"Reward: {episode_reward:.2f} - Step Count: {episode_step_count} - Run Time: {episode_run_time:.2f}s - Alpha: {self.alpha:.5f}\")\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_step_counts.append(episode_step_count)\n",
    "            episode_run_times.append(episode_run_time)\n",
    "            episode_alphas.append(self.alpha)\n",
    "\n",
    "            # Early stopping.\n",
    "            if stop_after is not None and all(ep_rew >= self.max_reward for ep_rew in episode_rewards[-stop_after:]):\n",
    "                break\n",
    "\n",
    "            if n % display_every == 0:\n",
    "                self.show_test_episode()\n",
    "\n",
    "        self.save_outputs(episode_rewards, episode_step_counts, episode_run_times, episode_alphas)"
   ],
   "id": "50fb56baa120e2b7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "63a7ea468b278b9f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class Actor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(24, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 256),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.mean_linear = nn.Linear(256, 4)\n",
    "        self.log_std_linear = nn.Linear(256, 4)\n",
    "\n",
    "        self.log_std_min = -20\n",
    "        self.log_std_max = 2\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.net(x)\n",
    "        mean = self.mean_linear(x)\n",
    "        log_std = self.log_std_linear(x)\n",
    "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
    "        return mean, log_std\n",
    "\n",
    "    def sample(self, x):\n",
    "        mean, log_std = self.forward(x)\n",
    "        std = log_std.exp()\n",
    "        normal = torch.distributions.Normal(mean, std)\n",
    "        a_raw = normal.rsample()\n",
    "        action = torch.tanh(a_raw)\n",
    "\n",
    "        log_prob = normal.log_prob(a_raw) - torch.log(1 - action ** 2 + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=-1, keepdim=True)\n",
    "        return action, log_prob"
   ],
   "id": "69493f7fd0446c31",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train robot.\n",
    "\n",
    "actor = Actor()\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=actor)\n",
    "\n",
    "agent.learn(stop_after=1)"
   ],
   "id": "2e2c406983365052",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Watch robot go in normal!\n",
    "\n",
    "trained_actor = Actor()\n",
    "trained_actor.load_state_dict(torch.load(\"SAC Outputs (normal)/actor_network.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=trained_actor)\n",
    "\n",
    "agent.show_test_episode()"
   ],
   "id": "246481ad902af31c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Watch robot go in hardcore!\n",
    "\n",
    "trained_actor = Actor()\n",
    "trained_actor.load_state_dict(torch.load(\"SAC Outputs (hardcore)/actor_network.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = SACAgent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=True,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=trained_actor)\n",
    "\n",
    "agent.show_test_episode()"
   ],
   "id": "5a1aace20072e6d9",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
