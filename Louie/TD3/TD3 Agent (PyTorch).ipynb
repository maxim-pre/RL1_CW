{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TD3 Implementation",
   "id": "57300d0a32eeeb02"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-08T21:12:49.527761Z",
     "start_time": "2025-12-08T21:12:49.493557Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import copy\n",
    "import gymnasium as gym\n",
    "import pickle\n",
    "import time"
   ],
   "id": "aa1e9096bf4290cd",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:12:50.875981Z",
     "start_time": "2025-12-08T21:12:50.799427Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TD3Agent:\n",
    "\n",
    "    def __init__(self,\n",
    "                 env_id,\n",
    "                 env_hardcore,\n",
    "                 critic_network1,\n",
    "                 critic_network2,\n",
    "                 actor_network,\n",
    "                 critic_learning_rate=1e-3,\n",
    "                 actor_learning_rate=1e-4,\n",
    "                 discount_factor=0.99,\n",
    "                 minibatch_size=256,\n",
    "                 tau=0.005,\n",
    "                 exploratory_noise=0.1,\n",
    "                 exploratory_noise_clip=0.3,\n",
    "                 policy_noise=0.2,\n",
    "                 policy_noise_clip=0.5,\n",
    "                 critic_gradient_clip=1.0,\n",
    "                 actor_gradient_clip=1.0,\n",
    "                 policy_delay=2,\n",
    "                 random_exploration_steps=10_000,\n",
    "                 actor_exploration_steps=0,\n",
    "                 updates_per_step=1,\n",
    "                 max_buffer_length=1_000_000):\n",
    "        # CPU or GPU?\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Environment.\n",
    "        self.env_id = env_id\n",
    "        self.env_hardcore = env_hardcore\n",
    "        self.env = gym.make(id=env_id, hardcore=env_hardcore, render_mode=None)\n",
    "        self.state_size = self.env.observation_space.shape[0]\n",
    "        self.action_size = self.env.action_space.shape[0]\n",
    "        self.action_min = torch.tensor(self.env.action_space.low, device=self.device)\n",
    "        self.action_max = torch.tensor(self.env.action_space.high, device=self.device)\n",
    "        self.max_reward = self.env.spec.reward_threshold\n",
    "\n",
    "        # Initialize critic network 1 and target critic network 1.\n",
    "        self.critic_network1 = copy.deepcopy(critic_network1)\n",
    "        self.target_critic_network1 = copy.deepcopy(critic_network1)\n",
    "\n",
    "        # Initialize critic network 2 and target critic network 2.\n",
    "        self.critic_network2 = copy.deepcopy(critic_network2)\n",
    "        self.target_critic_network2 = copy.deepcopy(critic_network2)\n",
    "\n",
    "        # Initialize actor network and target actor network.\n",
    "        self.actor_network = copy.deepcopy(actor_network)\n",
    "        self.target_actor_network = copy.deepcopy(actor_network)\n",
    "\n",
    "        # Move networks to correct device.\n",
    "        self.actor_network.to(self.device)\n",
    "        self.critic_network1.to(self.device)\n",
    "        self.critic_network2.to(self.device)\n",
    "        self.target_actor_network.to(self.device)\n",
    "        self.target_critic_network1.to(self.device)\n",
    "        self.target_critic_network2.to(self.device)\n",
    "\n",
    "        # Initialize optimizers.\n",
    "        self.critic_learning_rate = critic_learning_rate\n",
    "        self.actor_learning_rate = actor_learning_rate\n",
    "        self.critic1_optimizer = optim.Adam(self.critic_network1.parameters(), lr=critic_learning_rate)\n",
    "        self.critic2_optimizer = optim.Adam(self.critic_network2.parameters(), lr=critic_learning_rate)\n",
    "        self.actor_optimizer = optim.Adam(self.actor_network.parameters(), lr=actor_learning_rate)\n",
    "\n",
    "        # Initialize hyperparameters.\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.discount_factor = discount_factor\n",
    "        self.tau = tau\n",
    "        self.critic_gradient_clip = critic_gradient_clip\n",
    "        self.actor_gradient_clip = actor_gradient_clip\n",
    "        self.exploratory_noise = exploratory_noise\n",
    "        self.exploratory_noise_clip = exploratory_noise_clip\n",
    "        self.policy_noise = policy_noise\n",
    "        self.policy_noise_clip = policy_noise_clip\n",
    "        self.policy_delay = policy_delay\n",
    "        self.random_exploration_steps = random_exploration_steps\n",
    "        self.actor_exploration_steps = actor_exploration_steps\n",
    "        self.updates_per_step = updates_per_step\n",
    "\n",
    "        # Initialize buffer.\n",
    "        self.max_buffer_length = max_buffer_length\n",
    "        self.buffer_write_idx = 0\n",
    "        self.buffer_fullness = 0\n",
    "        self.buffer_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                         dtype=torch.float32,\n",
    "                                         device=self.device)\n",
    "        self.buffer_actions = torch.zeros((self.max_buffer_length, self.action_size),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_rewards = torch.zeros((self.max_buffer_length, 1),\n",
    "                                          dtype=torch.float32,\n",
    "                                          device=self.device)\n",
    "        self.buffer_next_states = torch.zeros((self.max_buffer_length, self.state_size),\n",
    "                                              dtype=torch.float32,\n",
    "                                              device=self.device)\n",
    "        self.buffer_dones = torch.zeros((self.max_buffer_length, 1),\n",
    "                                            dtype=torch.float32,\n",
    "                                            device=self.device)\n",
    "\n",
    "    def select_action(self, state: np.ndarray, add_noise=True) -> np.ndarray:\n",
    "        \"\"\"Select an action at the given state.\"\"\"\n",
    "        self.actor_network.eval()\n",
    "        with torch.no_grad():\n",
    "            # Forward pass.\n",
    "            state_tensor = torch.tensor(state, dtype=torch.float32, device=self.device).unsqueeze(0)\n",
    "            action_tensor = self.actor_network(state_tensor).squeeze(0)\n",
    "            # Add exploratory noise.\n",
    "            if add_noise:\n",
    "                noise = torch.randn(self.action_size, device=self.device) * self.exploratory_noise\n",
    "                noise_clipped = torch.clamp(noise, min=-self.exploratory_noise_clip, max=self.exploratory_noise_clip)\n",
    "                action_tensor = action_tensor + noise_clipped\n",
    "            # Clip action.\n",
    "            action_tensor = torch.clamp(action_tensor, min=self.action_min, max=self.action_max)\n",
    "            return action_tensor.cpu().numpy()\n",
    "\n",
    "    def save_transition(self, state: np.ndarray, action: np.ndarray, reward: float, new_state: np.ndarray, done: bool):\n",
    "        \"\"\"Save a transition.\"\"\"\n",
    "        # Save transition.\n",
    "        self.buffer_states[self.buffer_write_idx] = torch.tensor(state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_actions[self.buffer_write_idx] = torch.tensor(action, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_rewards[self.buffer_write_idx] = torch.tensor([reward], dtype=torch.float32, device=self.device)\n",
    "        self.buffer_next_states[self.buffer_write_idx] = torch.tensor(new_state, dtype=torch.float32, device=self.device)\n",
    "        self.buffer_dones[self.buffer_write_idx] = torch.tensor([1.0 if done else 0.0],\n",
    "                                                                    dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.buffer_write_idx = (self.buffer_write_idx + 1) % self.max_buffer_length\n",
    "        self.buffer_fullness = min(self.buffer_fullness + 1, self.max_buffer_length)\n",
    "\n",
    "    def sample_minibatch(self):\n",
    "        \"\"\"Sample a minibatch from the replay buffer.\"\"\"\n",
    "        indices = torch.randint(0, self.buffer_fullness, (self.minibatch_size,), device=self.device)\n",
    "\n",
    "        mb_states = self.buffer_states[indices]\n",
    "        mb_actions = self.buffer_actions[indices]\n",
    "        mb_rewards = self.buffer_rewards[indices]\n",
    "        mb_next_states = self.buffer_next_states[indices]\n",
    "        mb_dones = self.buffer_dones[indices]\n",
    "\n",
    "        return mb_states, mb_actions, mb_rewards, mb_next_states, mb_dones\n",
    "\n",
    "    def update_critic_networks(self, minibatch: torch.Tensor):\n",
    "        \"\"\"Update critic networks\"\"\"\n",
    "        mb_states, mb_actions, mb_rewards, mb_next_states, mb_dones = minibatch\n",
    "        mb_state_actions = torch.cat([mb_states, mb_actions], dim=1)\n",
    "\n",
    "        self.target_actor_network.eval()\n",
    "        self.target_critic_network1.eval()\n",
    "        self.target_critic_network2.eval()\n",
    "        self.critic_network1.train()\n",
    "        self.critic_network2.train()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.target_actor_network(mb_next_states)\n",
    "            noise = torch.randn(next_actions.shape, device=self.device) * self.policy_noise\n",
    "            noise = torch.clamp(noise, min=-self.policy_noise_clip, max=self.policy_noise_clip)\n",
    "            next_actions = torch.clamp(next_actions + noise, min=self.action_min, max=self.action_max)\n",
    "            next_state_actions = torch.cat((mb_next_states, next_actions), dim=1)\n",
    "\n",
    "            q1_next = self.target_critic_network1(next_state_actions)\n",
    "            q2_next = self.target_critic_network2(next_state_actions)\n",
    "            q_min_next = torch.min(q1_next, q2_next)\n",
    "            q_target = mb_rewards + self.discount_factor * (1 - mb_dones) * q_min_next\n",
    "\n",
    "        # Critic network 1 update.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network1(mb_state_actions)\n",
    "            critic1_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic1_optimizer.zero_grad()\n",
    "        critic1_loss.backward()\n",
    "        if self.critic_gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.critic_network1.parameters(), self.critic_gradient_clip)\n",
    "        self.critic1_optimizer.step()\n",
    "\n",
    "        # Critic network 2 update.\n",
    "        with torch.set_grad_enabled(True):\n",
    "            q_expected = self.critic_network2(mb_state_actions)\n",
    "            critic2_loss = torch.mean((q_target - q_expected) ** 2)\n",
    "\n",
    "        self.critic2_optimizer.zero_grad()\n",
    "        critic2_loss.backward()\n",
    "        if self.critic_gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.critic_network2.parameters(), self.critic_gradient_clip)\n",
    "        self.critic2_optimizer.step()\n",
    "\n",
    "    def update_actor_network(self, minibatch: torch.Tensor):\n",
    "        \"\"\"Update the actor network.\"\"\"\n",
    "        mb_states, *_ = minibatch\n",
    "\n",
    "        self.actor_network.train()\n",
    "        self.critic_network1.eval()\n",
    "\n",
    "        with torch.set_grad_enabled(True):\n",
    "            raw_actions = self.actor_network(mb_states)\n",
    "            raw_state_actions = torch.cat((mb_states, raw_actions), dim=1)\n",
    "            actor_loss = -self.critic_network1(raw_state_actions).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        if self.actor_gradient_clip > 0.0:\n",
    "            nn.utils.clip_grad_norm_(self.actor_network.parameters(), self.actor_gradient_clip)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "    def soft_update_target_critics(self):\n",
    "        with torch.no_grad():\n",
    "            for w_target, w_local in zip(self.target_critic_network1.parameters(), self.critic_network1.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "            for w_target, w_local in zip(self.target_critic_network2.parameters(), self.critic_network2.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "    def soft_update_target_actor(self):\n",
    "        with torch.no_grad():\n",
    "            for w_target, w_local in zip(self.target_actor_network.parameters(), self.actor_network.parameters()):\n",
    "                w_target.data.copy_(self.tau * w_local.data + (1 - self.tau) * w_target.data)\n",
    "\n",
    "    def get_settings(self, n_episodes, stop_after):\n",
    "        return {\n",
    "            \"env_id\": self.env_id,\n",
    "            \"env_hardcore\": self.env_hardcore,\n",
    "            \"critic_learning_rate\": self.critic_learning_rate,\n",
    "            \"actor_learning_rate\": self.actor_learning_rate,\n",
    "            \"discount_factor\": self.discount_factor,\n",
    "            \"minibatch_size\": self.minibatch_size,\n",
    "            \"tau\": self.tau,\n",
    "            \"exploratory_noise\": self.exploratory_noise,\n",
    "            \"exploratory_noise_clip\": self.exploratory_noise_clip,\n",
    "            \"policy_noise\": self.policy_noise,\n",
    "            \"policy_noise_clip\": self.policy_noise_clip,\n",
    "            \"policy_delay\": self.policy_delay,\n",
    "            \"random_exploration_steps\": self.random_exploration_steps,\n",
    "            \"actor_exploration_steps\": self.actor_exploration_steps,\n",
    "            \"critic_gradient_clip\": self.critic_gradient_clip,\n",
    "            \"actor_gradient_clip\": self.actor_gradient_clip,\n",
    "            \"updates_per_step\": self.updates_per_step,\n",
    "            \"max_buffer_length\": self.max_buffer_length,\n",
    "            \"n_episodes\": n_episodes,\n",
    "            \"stop_after\": stop_after\n",
    "        }\n",
    "\n",
    "    def save_outputs(self, episode_rewards, episode_step_counts, episode_run_times, n_episodes, stop_after):\n",
    "        \"\"\"Save each network's weights and episode rewards.\"\"\"\n",
    "        torch.save(self.critic_network1.state_dict(), \"critic_network1.pth\")\n",
    "        torch.save(self.critic_network2.state_dict(), \"critic_network2.pth\")\n",
    "        torch.save(self.actor_network.state_dict(), \"actor_network.pth\")\n",
    "        with open(\"episode_rewards.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_rewards, fp)\n",
    "        with open(\"episode_step_counts.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_step_counts, fp)\n",
    "        with open(\"episode_run_times.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(episode_run_times, fp)\n",
    "        with open(\"settings.pkl\", \"wb\") as fp:\n",
    "            pickle.dump(self.get_settings(n_episodes, stop_after), fp)\n",
    "\n",
    "    def show_test_episode(self):\n",
    "        \"\"\"Do a visual test run.\"\"\"\n",
    "        print(\"\\n========TEST RUN========\")\n",
    "        test_env = gym.make(id=self.env_id, hardcore=self.env_hardcore, render_mode=\"human\")\n",
    "        s, _ = test_env.reset()\n",
    "        test_episode_reward = 0\n",
    "        test_episode_step_count = 0\n",
    "        test_start_time = time.time()\n",
    "\n",
    "        while True:\n",
    "            a = self.select_action(s, add_noise=False)\n",
    "            s_, r, terminal, truncated, _ = test_env.step(a)\n",
    "            done = terminal or truncated\n",
    "\n",
    "            test_episode_reward += r\n",
    "            test_episode_step_count += 1\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            s = s_\n",
    "\n",
    "        test_episode_end_time = time.time()\n",
    "        test_episode_run_time = test_episode_end_time - test_start_time\n",
    "        test_env.close()\n",
    "\n",
    "        print(f\"Reward: {test_episode_reward:.2f} - Step Count: {test_episode_step_count} - Run Time: {test_episode_run_time:.2f}s\\n\")\n",
    "\n",
    "    def random_exploration(self):\n",
    "        print(\"Performing Random Exploration...\")\n",
    "        step_count = 0\n",
    "        state, _ = self.env.reset()\n",
    "\n",
    "        while step_count < self.random_exploration_steps:\n",
    "\n",
    "            action_tensor = self.action_min + (self.action_max - self.action_min) * torch.rand(self.action_size, device=self.device)\n",
    "            action = action_tensor.cpu().numpy()\n",
    "            new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "            done = terminal or truncated\n",
    "            self.save_transition(state, action, reward, new_state, done)\n",
    "\n",
    "            step_count += 1\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "\n",
    "        print(\"Random Exploration Complete.\")\n",
    "\n",
    "    def actor_exploration(self):\n",
    "        print(\"Performing Actor Exploration...\")\n",
    "        step_count = 0\n",
    "        state, _ = self.env.reset()\n",
    "\n",
    "        while step_count < self.actor_exploration_steps:\n",
    "\n",
    "            action = self.select_action(state)\n",
    "            new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "            done = terminal or truncated\n",
    "            self.save_transition(state, action, reward, new_state, done)\n",
    "\n",
    "            step_count += 1\n",
    "            state = new_state\n",
    "\n",
    "            if done:\n",
    "                state, _ = self.env.reset()\n",
    "\n",
    "        print(\"Actor Exploration Complete.\")\n",
    "\n",
    "    def learn(self, n_episodes=2000, display_every=50, stop_after=None):\n",
    "\n",
    "        self.random_exploration()\n",
    "        self.actor_exploration()\n",
    "\n",
    "        episode_rewards = []\n",
    "        episode_step_counts = []\n",
    "        episode_run_times = []\n",
    "\n",
    "        total_step_count = 0\n",
    "\n",
    "        for n in range(n_episodes):\n",
    "            # Print episode number.\n",
    "            print(f\"Running Episode {n + 1}...\")\n",
    "\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Reset environment.\n",
    "            state, _ = self.env.reset()\n",
    "\n",
    "            episode_reward = 0\n",
    "            episode_step_count = 0\n",
    "\n",
    "            while True:\n",
    "                # Select action and take step.\n",
    "                action = self.select_action(state)\n",
    "                new_state, reward, terminal, truncated, _ = self.env.step(action)\n",
    "                done = terminal or truncated\n",
    "\n",
    "                # Store transition.\n",
    "                self.save_transition(state, action, reward, new_state, done)\n",
    "\n",
    "                # Update counts.\n",
    "                episode_step_count += 1\n",
    "                total_step_count += 1\n",
    "\n",
    "                # Update rewards.\n",
    "                episode_reward += reward\n",
    "\n",
    "                if self.buffer_fullness >= self.minibatch_size:\n",
    "\n",
    "                    for _ in range(self.updates_per_step):\n",
    "\n",
    "                        minibatch = self.sample_minibatch()\n",
    "                        self.update_critic_networks(minibatch)\n",
    "\n",
    "                        if total_step_count % self.policy_delay == 0:\n",
    "\n",
    "                            self.update_actor_network(minibatch)\n",
    "                            self.soft_update_target_critics()\n",
    "                            self.soft_update_target_actor()\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "                state = new_state\n",
    "\n",
    "            end_time = time.time()\n",
    "            episode_run_time = end_time - start_time\n",
    "\n",
    "            # Print and save episode reward.\n",
    "            print(f\"Reward: {episode_reward:.2f} - Step Count: {episode_step_count} - Run Time: {episode_run_time:.2f}s - Total Step Count - {total_step_count}\")\n",
    "            episode_rewards.append(episode_reward)\n",
    "            episode_step_counts.append(episode_step_count)\n",
    "            episode_run_times.append(episode_run_time)\n",
    "\n",
    "            # Early stopping.\n",
    "            if stop_after is not None and all(ep_rew >= self.max_reward for ep_rew in episode_rewards[-stop_after:]):\n",
    "                break\n",
    "\n",
    "            if n % display_every == 0:\n",
    "                self.show_test_episode()\n",
    "\n",
    "        self.save_outputs(episode_rewards, episode_step_counts, episode_run_times, n_episodes, stop_after)"
   ],
   "id": "e53644611ff62c1d",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:13:03.657134Z",
     "start_time": "2025-12-08T21:13:03.650111Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(28, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "857045117c0b1f95",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:13:05.237577Z",
     "start_time": "2025-12-08T21:13:05.208816Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(24, 400),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(400, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 4),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ],
   "id": "ae27c0fc1754edad",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training - Complete Normal Mode",
   "id": "199da62d9ac4bfca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Train the robot.\n",
    "\n",
    "actor = Actor()\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = TD3Agent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=actor)\n",
    "\n",
    "agent.learn(n_episodes=2000, stop_after=1)"
   ],
   "id": "b87e24a317c9ed6e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-08T21:13:17.584657Z",
     "start_time": "2025-12-08T21:13:17.558954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Print hyperparameters used.\n",
    "\n",
    "with open(\"outputs/settings.pkl\", \"rb\") as f:\n",
    "    settings = pickle.load(f)\n",
    "\n",
    "print(settings)"
   ],
   "id": "50eb3047769dd29a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'env_id': 'BipedalWalker-v3', 'env_hardcore': False, 'critic_learning_rate': 0.001, 'actor_learning_rate': 0.0001, 'discount_factor': 0.99, 'minibatch_size': 256, 'tau': 0.005, 'exploratory_noise': 0.1, 'exploratory_noise_clip': 0.3, 'policy_noise': 0.2, 'policy_noise_clip': 0.5, 'policy_delay': 2, 'random_exploration_steps': 10000, 'actor_exploration_steps': 0, 'critic_gradient_clip': 1.0, 'actor_gradient_clip': 1.0, 'updates_per_step': 1, 'max_buffer_length': 1000000, 'n_episodes': 2000, 'stop_after': 1}\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T08:39:34.117665Z",
     "start_time": "2025-12-09T08:39:15.152346Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Watch robot go!\n",
    "\n",
    "trained_actor = Actor()\n",
    "trained_actor.load_state_dict(torch.load(\"outputs/actor_network.pth\", map_location=torch.device(\"cpu\")))\n",
    "\n",
    "critic1 = Critic()\n",
    "critic2 = Critic()\n",
    "\n",
    "agent = TD3Agent(env_id=\"BipedalWalker-v3\",\n",
    "                 env_hardcore=False,\n",
    "                 critic_network1=critic1,\n",
    "                 critic_network2=critic2,\n",
    "                 actor_network=trained_actor)\n",
    "\n",
    "agent.show_test_episode()"
   ],
   "id": "cb89747d686e5e4c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========TEST RUN========\n",
      "Reward: 299.56 - Step Count: 861 - Run Time: 17.76s\n",
      "\n"
     ]
    }
   ],
   "execution_count": 19
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
