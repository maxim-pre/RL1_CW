{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-11T10:42:13.630538Z",
     "start_time": "2025-12-11T10:42:11.596576Z"
    }
   },
   "source": [
    "import sys\n",
    "import torch\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "    from google.colab import drive\n",
    "    drive.mount(\"/content/drive\")\n",
    "    %cd \"/content/drive/MyDrive/Python/Bath University/RL1_CW/Louie/DDPG\"\n",
    "    !pip install swig\n",
    "    !pip install gymnasium[box2d]\n",
    "\n",
    "from Agent import DDPGAgent\n",
    "from Networks import Critic, Actor\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Training - Complete Normal Mode",
   "id": "75475b3e36a68729"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T10:42:58.243589Z",
     "start_time": "2025-12-11T10:42:33.337973Z"
    }
   },
   "cell_type": "code",
   "source": [
    "actor = Actor()\n",
    "critic = Critic()\n",
    "\n",
    "agent = DDPGAgent(critic_network=critic,\n",
    "                  actor_network=actor,\n",
    "                  device=device,\n",
    "                  hardcore=False,\n",
    "                  max_buffer_length=1_000_000)\n",
    "\n",
    "agent.learn(n_episodes=2000,\n",
    "            discount_factor=0.99,\n",
    "            minibatch_size=256,\n",
    "            tau=0.005,\n",
    "            random_exploration_steps=10_000,\n",
    "            actor_exploration_steps=0,\n",
    "            vid_every=50,\n",
    "            stop_after=1,\n",
    "            reset_optim=True,\n",
    "            reset_buffer=True,\n",
    "            # ====================== #\n",
    "            critic_lr=1e-3,\n",
    "            actor_lr=1e-4,\n",
    "            critic_grad_clip=1.0,\n",
    "            actor_grad_clip=1.0,\n",
    "            exploratory_noise_start=0.3,\n",
    "            exploratory_noise_min=0.05,\n",
    "            exploratory_noise_decay=2.5e-7,\n",
    "            exploratory_noise_clip=0.3,\n",
    "            updates_per_step=1)"
   ],
   "id": "b6f780c9382ed264",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louie\\Python Environments\\RL1_CW\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing Random Exploration...\n",
      "Performing Actor Exploration...\n",
      "Running Episode 1...\n",
      "Reward: -113.02 - Step Count: 274 - Run Time: 3.86s - Total Step Count - 274\n",
      "0.29064975000000004\n",
      "\n",
      "========TEST RUN========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\louie\\Python Environments\\RL1_CW\\Lib\\site-packages\\gymnasium\\wrappers\\rendering.py:293: UserWarning: \u001B[33mWARN: Overwriting existing videos at C:\\Users\\louie\\OneDrive - University of Bath\\CM52074 (RL) Notebooks\\RL1_CW\\Louie 2\\DDPG\\videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001B[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: -115.14 - Step Count: 93 - Run Time: 0.59s\n",
      "\n",
      "Running Episode 2...\n",
      "Reward: -124.65 - Step Count: 91 - Run Time: 1.47s - Total Step Count - 365\n",
      "0.28339250000000005\n",
      "Running Episode 3...\n",
      "Reward: -108.29 - Step Count: 227 - Run Time: 3.63s - Total Step Count - 592\n",
      "0.256266\n",
      "Running Episode 4...\n",
      "Reward: -117.32 - Step Count: 54 - Run Time: 0.88s - Total Step Count - 646\n",
      "0.24791625000000003\n",
      "Running Episode 5...\n",
      "Reward: -117.05 - Step Count: 47 - Run Time: 0.72s - Total Step Count - 693\n",
      "0.2400555\n",
      "Running Episode 6...\n",
      "Reward: -105.21 - Step Count: 67 - Run Time: 1.05s - Total Step Count - 760\n",
      "0.22789500000000001\n",
      "Running Episode 7...\n",
      "Reward: -110.50 - Step Count: 50 - Run Time: 0.81s - Total Step Count - 810\n",
      "0.21808875000000003\n",
      "Running Episode 8...\n",
      "Reward: -115.11 - Step Count: 47 - Run Time: 0.75s - Total Step Count - 857\n",
      "0.20830100000000001\n",
      "Running Episode 9...\n",
      "Reward: -109.39 - Step Count: 125 - Run Time: 2.10s - Total Step Count - 982\n",
      "0.17958225000000003\n",
      "Running Episode 10...\n",
      "Reward: -113.18 - Step Count: 87 - Run Time: 1.30s - Total Step Count - 1069\n",
      "0.15728850000000003\n",
      "Running Episode 11...\n",
      "Reward: -115.91 - Step Count: 85 - Run Time: 1.38s - Total Step Count - 1154\n",
      "0.13367975000000004\n",
      "Running Episode 12...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mKeyboardInterrupt\u001B[39m                         Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[2]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      2\u001B[39m critic = Critic()\n\u001B[32m      4\u001B[39m agent = DDPGAgent(critic_network=critic,\n\u001B[32m      5\u001B[39m                   actor_network=actor,\n\u001B[32m      6\u001B[39m                   device=device,\n\u001B[32m      7\u001B[39m                   hardcore=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m      8\u001B[39m                   max_buffer_length=\u001B[32m1_000_000\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[43magent\u001B[49m\u001B[43m.\u001B[49m\u001B[43mlearn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mn_episodes\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     11\u001B[39m \u001B[43m            \u001B[49m\u001B[43mdiscount_factor\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.99\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     12\u001B[39m \u001B[43m            \u001B[49m\u001B[43mminibatch_size\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m256\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     13\u001B[39m \u001B[43m            \u001B[49m\u001B[43mtau\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.005\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     14\u001B[39m \u001B[43m            \u001B[49m\u001B[43mrandom_exploration_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m10_000\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     15\u001B[39m \u001B[43m            \u001B[49m\u001B[43mactor_exploration_steps\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     16\u001B[39m \u001B[43m            \u001B[49m\u001B[43mvid_every\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m50\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     17\u001B[39m \u001B[43m            \u001B[49m\u001B[43mstop_after\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     18\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreset_optim\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     19\u001B[39m \u001B[43m            \u001B[49m\u001B[43mreset_buffer\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m     20\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;66;43;03m# ====================== #\u001B[39;49;00m\n\u001B[32m     21\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcritic_lr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     22\u001B[39m \u001B[43m            \u001B[49m\u001B[43mactor_lr\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1e-4\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     23\u001B[39m \u001B[43m            \u001B[49m\u001B[43mcritic_grad_clip\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     24\u001B[39m \u001B[43m            \u001B[49m\u001B[43mactor_grad_clip\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1.0\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     25\u001B[39m \u001B[43m            \u001B[49m\u001B[43mexploratory_noise_start\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     26\u001B[39m \u001B[43m            \u001B[49m\u001B[43mexploratory_noise_min\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.05\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     27\u001B[39m \u001B[43m            \u001B[49m\u001B[43mexploratory_noise_decay\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m2.5e-7\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     28\u001B[39m \u001B[43m            \u001B[49m\u001B[43mexploratory_noise_clip\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m0.3\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     29\u001B[39m \u001B[43m            \u001B[49m\u001B[43mupdates_per_step\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - University of Bath\\CM52074 (RL) Notebooks\\RL1_CW\\Louie 2\\DDPG\\Agent.py:365\u001B[39m, in \u001B[36mDDPGAgent.learn\u001B[39m\u001B[34m(self, n_episodes, discount_factor, minibatch_size, tau, random_exploration_steps, actor_exploration_steps, vid_every, stop_after, reset_optim, reset_buffer, critic_lr, actor_lr, critic_grad_clip, actor_grad_clip, exploratory_noise_start, exploratory_noise_min, exploratory_noise_decay, exploratory_noise_clip, updates_per_step)\u001B[39m\n\u001B[32m    360\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m done:\n\u001B[32m    362\u001B[39m     exploratory_noise = \u001B[38;5;28mmax\u001B[39m(exploratory_noise_min,\n\u001B[32m    363\u001B[39m                             exploratory_noise - exploratory_noise_decay * total_step_count)\n\u001B[32m--> \u001B[39m\u001B[32m365\u001B[39m     action = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mselect_action\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    366\u001B[39m \u001B[43m                                \u001B[49m\u001B[43madd_noise\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[32m    367\u001B[39m \u001B[43m                                \u001B[49m\u001B[43mexploratory_noise\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexploratory_noise\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    368\u001B[39m \u001B[43m                                \u001B[49m\u001B[43mexploratory_noise_clip\u001B[49m\u001B[43m=\u001B[49m\u001B[43mexploratory_noise_clip\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    369\u001B[39m     new_state, reward, terminal, truncated, _ = \u001B[38;5;28mself\u001B[39m.env.step(action)\n\u001B[32m    370\u001B[39m     done = terminal \u001B[38;5;129;01mor\u001B[39;00m truncated\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\OneDrive - University of Bath\\CM52074 (RL) Notebooks\\RL1_CW\\Louie 2\\DDPG\\Agent.py:85\u001B[39m, in \u001B[36mDDPGAgent.select_action\u001B[39m\u001B[34m(self, state, add_noise, exploratory_noise, exploratory_noise_clip)\u001B[39m\n\u001B[32m     83\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Select an action at the given state.\"\"\"\u001B[39;00m\n\u001B[32m     84\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m torch.no_grad():\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m     state_tensor = \u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtorch\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfloat32\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m=\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m.unsqueeze(\u001B[32m0\u001B[39m)\n\u001B[32m     86\u001B[39m     action_tensor = \u001B[38;5;28mself\u001B[39m.actor_network(state_tensor).squeeze(\u001B[32m0\u001B[39m)\n\u001B[32m     87\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m add_noise:\n",
      "\u001B[31mKeyboardInterrupt\u001B[39m: "
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
